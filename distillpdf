#!/usr/bin/env python3
"""
DistillPDF: Advanced PDF Optimization and Transformation Tool

This script provides comprehensive functionalities to optimize and transform
PDF files. It includes capabilities for resizing and compressing images within
PDFs, converting images to grayscale, flattening form fields and annotations,
and aggressively subsetting embedded fonts to reduce file size.
Vector graphics within PDFs are optimized for minimal visual alteration using
Bayesian optimization techniques. Additional functionalities include removing
metadata for privacy, cleaning unused resources, and optimizing the entire
content stream of PDF pages.

Features:
    - Image resizing and compression with configurable dimensions and quality.
    - Conversion of colored images to grayscale to reduce complexity.
    - Flattening of PDF annotations and form fields to static content.
    - Aggressive font subsetting to include only the characters used in the document.
    - Metadata removal for enhanced privacy and security.
    - Cleanup of unreferenced PDF resources to shrink file size.
    - Content stream optimization to strip unnecessary data.
    - Vector graphic simplification using Bayesian optimization for optimal results.
    - Text-aware and shape-aware vector optimization to preserve important content.
    - Multi-threaded processing for performance optimization.
    - Parallel batch processing of multiple PDFs from a directory.
    - Adaptive concurrency management to handle system file descriptor limits.
    - Extreme optimization mode for maximum file size reduction.
    - Detailed logging of processes and errors for troubleshooting and analysis.

Benefits for Vector Embeddings and RAG Systems:
    The optimizations performed by this script can offer benefits for creating
    vector embeddings and using PDFs in Retrieval-Augmented Generation (RAG) systems:
    
    - Processing Speed: Smaller file sizes may lead to faster text extraction
      for embedding creation, especially beneficial for large PDF collections.
    - Storage Efficiency: Reduced file sizes allow storing more documents in
      vector databases, potentially improving the knowledge breadth for RAG systems.
    - Cleaner Text Extraction: Flattening annotations and form fields may result
      in cleaner text extraction, potentially improving embedding quality.
    - Noise Reduction: Cleaning unreferenced resources might remove noise that
      could otherwise be captured in the embedding process.

The script utilizes a combination of libraries including PyMuPDF (fitz), pikepdf,
OpenCV, scikit-learn, scipy, and numpy to perform its operations, integrating
advanced image processing, machine learning for optimization decisions, and direct
PDF manipulation capabilities.

Setup:
    Ensure you have Python version >= 3.9 installed.
    
    $ pip install -r requirements.txt
    $ chmod +x distillpdf
    
    (Recommended) Move the distillpdf script to a directory in your PATH, and modify
    this file's shebang to point to your compatible Python binary location.

Usage:
    To use the script, provide an input PDF file or directory and an output destination, along with
    optional flags for grayscale conversion, flattening, and specifying the number of
    processing threads. The script is designed to be run as a command-line tool.

Example:
    $ ./distillpdf input.pdf --output output.pdf --threads=4 --grayscale --flatten
    $ ./distillpdf input_directory/ --output_dir=optimized_pdfs/ --threads=4 --grayscale --flatten

Design:
    This script is designed for high-performance environments where precision and file
    size reduction are critical, suitable for both commercial and academic applications.
    It employs adaptive concurrency management and Bayesian optimization to balance
    processing speed with system resource constraints and optimization quality.

Author:
    Juan Sugg Gilbert
    juanpedrosugg [at] gmail [dot] com

License:
    This script is licensed under the GNU Affero General Public License v3.0
"""
from multiprocessing.sharedctypes import Synchronized
from multiprocessing.synchronize import Semaphore
import os
import sys
import traceback
import warnings
import io
import subprocess
import argparse
import tempfile
import logging
import shutil
from typing import Any, List, Tuple, Set, Optional, Callable
import math
import random
import concurrent.futures
from functools import lru_cache, wraps
import multiprocessing
from multiprocessing.pool import ApplyResult
import concurrent.futures
import time
import resource
import psutil

import numpy as np
import cv2
from cv2.typing import MatLike
from scipy.stats import norm
from scipy.optimize import minimize, OptimizeResult
from sklearn.exceptions import ConvergenceWarning
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, Sum, WhiteKernel
from fontTools import subset
from fontTools.ttLib import TTFont
import pikepdf
import fitz  # PyMuPDF

MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds
MAX_FD_USAGE_PERCENT = 80  # maximum percentage of file descriptors to use

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

warnings.simplefilter("ignore", ConvergenceWarning)


class PDFProcessingError(Exception):
    """Base class for exceptions in the Distill PDF module."""

    def __init__(self, message: str, context: str) -> None:
        """
        Initialize the PDFProcessingError.

        Args:
            message (str): The error message.
            context (str): Additional context about the error.
        """
        super().__init__(f"{message} - Context: {context}")
        self.context: str = context

    def get_message_chain(self) -> List[str]:
        """
        Return a list of messages representing the exception chain.

        Returns:
            List[str]: A list of error messages.
        """
        messages: List[str] = [str(self)]
        cause: Optional[BaseException] = self.__cause__
        while cause:
            if isinstance(cause, PDFProcessingError):
                messages.extend(cause.get_message_chain())
            else:
                messages.append(str(cause))
            cause = getattr(cause, "__cause__", None)
        return messages


class FileAccessError(PDFProcessingError):
    """Exception raised for errors accessing input or output files."""


class TextDetectionError(PDFProcessingError):
    """Exception raised for errors during text detection in images."""


class ImageProcessingError(PDFProcessingError):
    """Exception raised for errors during image processing."""


class RasterOptimizationError(PDFProcessingError):
    """Exception raised for errors during raster image optimization."""


class VectorOptimizationError(PDFProcessingError):
    """Exception raised for errors during vector optimization."""


class FontSubsettingError(PDFProcessingError):
    """Exception raised for errors during font subsetting."""


class ContentOptimizationError(PDFProcessingError):
    """Exception raised for errors during content stream optimization."""


## File Management

class AdaptiveSemaphore:
    def __init__(self, initial_value: int, max_value: int) -> None:
        self.semaphore: Semaphore = multiprocessing.Semaphore(initial_value)
        self.max_value: int = max_value
        self.current_value: Synchronized[Any] = multiprocessing.Value(
            "i", initial_value
        )

    def acquire(self) -> None:
        self.semaphore.acquire()
        with self.current_value.get_lock():
            self.current_value.value -= 1

    def release(self) -> None:
        self.semaphore.release()
        with self.current_value.get_lock():
            self.current_value.value += 1

    def get_current_value(self) -> Any:
        return self.current_value.value

    def update_max_value(self, new_max: int) -> None:
        diff: int = new_max - self.max_value
        self.max_value = new_max
        for _ in range(diff):
            self.semaphore.release()
            with self.current_value.get_lock():
                self.current_value.value += 1


def with_adaptive_semaphore(func):
    @wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        file_semaphore.acquire()
        try:
            return func(*args, **kwargs)
        finally:
            file_semaphore.release()

    return wrapper


def get_open_files_count() -> int:
    return len(psutil.Process().open_files())


def update_semaphore_value() -> None:
    current_open_files: int = get_open_files_count()
    _, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE)
    available_fds: int = hard_limit - current_open_files
    max_concurrent: int = min(
        int(available_fds * MAX_FD_USAGE_PERCENT / 100), cli_args.max_files
    )
    file_semaphore.update_max_value(max_concurrent)
    logging.info("Updated max concurrent files to: %d", max_concurrent)


def safe_file_operation(func: Any) -> Any:
    """Decorator to safely handle file operations."""

    def wrapper(*args: Any, **kwargs: Any) -> Any:
        try:
            return func(*args, **kwargs)
        except OSError as e:
            raise FileAccessError(
                f"File operation failed: {func.__name__}", str(e)
            ) from e

    return wrapper


@safe_file_operation
def create_temp_file(suffix: str) -> tempfile._TemporaryFileWrapper:
    """
    Safely create a named temporary file.

    Args:
        suffix (str): The file suffix.

    Returns:
        tempfile._TemporaryFileWrapper: A temporary file object.
    """
    return tempfile.NamedTemporaryFile(suffix=suffix, delete=False)


## Image Optimization


def optimize_raster_image(
    image_bytes: bytes, max_width: int = 1000, quality: int = 75
) -> bytes:
    """
    Optimize a raster image by resizing and compressing it using JPEG.

    Args:
        image_bytes (bytes): The input image bytes.
        max_width (int, optional): The maximum width of the optimized image.
                                   Defaults to 1000.
        quality (int, optional): The JPEG quality (0-100). Defaults to 75.

    Returns:
        bytes: The optimized image bytes.

    Raises:
        ImageProcessingError: If image optimization fails.
    """
    try:
        # pylint: disable=no-member
        nparr: np.ndarray = np.frombuffer(image_bytes, np.uint8)
        img: MatLike = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)
        h, w = img.shape[:2]
        if w > max_width:
            ratio: float = max_width / w
            new_h = int(h * ratio)
            img: MatLike = cv2.resize(
                img, (max_width, new_h), interpolation=cv2.INTER_AREA
            )

        _, optimized_img = cv2.imencode(
            ".jpg", img, [cv2.IMWRITE_JPEG_QUALITY, quality]
        )
        # pylint: enable=no-member
        if optimized_img is None:
            raise ImageProcessingError(
                "Failed to encode optimized image", "cv2.imencode"
            )
        return optimized_img.tobytes()
    except Exception as e:
        raise ImageProcessingError(
            "Error optimizing raster image", str(e)
        ) from e


def get_image_in_bytes_representation(image: fitz.Pixmap) -> bytes:
    """
    Get the byte representation of an image in PNG format.

    Args:
        image (fitz.Pixmap): The input image.

    Returns:
        bytes: The image bytes in PNG format.

    Raises:
        ImageProcessingError: If conversion to bytes fails.
    """
    try:
        return image.tobytes("png")
    except Exception as e:
        raise ImageProcessingError(
            "Failed to convert image to bytes", str(e)
        ) from e


def is_rect_finite(rect: fitz.Rect) -> bool:
    """
    Check if all coordinates of a rectangle are finite.

    Args:
        rect (fitz.Rect): The rectangle to check.

    Returns:
        bool: True if all coordinates are finite, False otherwise.
    """
    return all(
        math.isfinite(coord) for coord in [rect.x0, rect.y0, rect.x1, rect.y1]
    )


def get_image_rect(page: fitz.Page, img_xref: int) -> Optional[fitz.Rect]:
    """
    Get the rectangle of an image on a PDF page.

    Args:
        page (fitz.Page): The PDF page.
        img_xref (int): The image reference.

    Returns:
        Optional[fitz.Rect]: The image rectangle, or None if not found.

    Raises:
        ImageProcessingError: If image rectangle calculation fails.
    """
    try:
        img_list: List[Tuple] = page.get_images(full=True)
        for img in img_list:
            if img[0] == img_xref:
                img_rect: fitz.Rect = page.get_image_bbox(img)  # type: ignore # Given the current arguments, this is safe
                if img_rect.is_empty or img_rect.is_infinite:
                    logging.debug(
                        "get_image_rect -> Image rectangle is empty or not finite, skipping."
                    )
                    return None
                return img_rect
        return None
    except Exception as e:
        logging.error(
            "Failed to calculate image rectangle for xref %d: %s",
            img_xref,
            str(e),
        )
        raise ImageProcessingError(
            f"Failed to get image rectangle for xref {img_xref}", str(e)
        ) from e


def _insert_optimized_image(img_xref, img_rect, image_bytes, page) -> None:
    """
    Inserts an optimized image into a PDF page.

    Args:
        img_xref (int): The xref of the image.
        img_rect (Rectangle): The rectangle where the image will be inserted.
        image_bytes (bytes): The bytes of the image to be inserted.
        page: The PDF page to insert the image into.

    Returns:
        None

    Raises:
        ValueError: If the rectangle is not finite or empty.
    """
    logging.debug("Image rect for xref %s: %s", img_xref, img_rect)

    if not is_rect_finite(img_rect):
        raise ValueError("Rectangle is not finite.")

    if img_rect.is_empty:
        raise ValueError("Rectangle is empty.")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp_file:
        temp_file.write(image_bytes)
        temp_file.flush()
        temp_file_path = temp_file.name

    page.insert_image(img_rect, filename=temp_file.name, keep_proportion=True)  # type: ignore # pymupdf library does not expose this method

    os.unlink(temp_file_path)


def replace_image_with_optimized_version(
    page: fitz.Page, image_bytes: bytes, img_rect: fitz.Rect, img_xref: int
) -> None:
    """
    Replace the image with an optimized version using JPEG.

    Args:
        page (fitz.Page): The PDF page.
        image_bytes (bytes): The optimized image bytes.
        img_rect (fitz.Rect): The image rectangle.
        img_xref (int): The image reference.

    Raises:
        ValueError: If the image rectangle is invalid.
    """
    try:
        _insert_optimized_image(img_xref, img_rect, image_bytes, page)
    except Exception as e:
        logging.debug(
            "Failed to insert or delete image with xref = %d: %s",
            img_xref,
            str(e),
        )
        raise PDFProcessingError(
            f"Failed to insert or delete image with xref {img_xref}", str(e)
        ) from e

    try:
        page.delete_image(img_xref)  # type: ignore  # pymupdf library does not expose this method
    except Exception as e:
        logging.debug(
            "Failed to delete the original image with xref = %d: %s",
            img_xref,
            str(e),
        )
        raise PDFProcessingError(
            f"Failed to delete the original image with xref {img_xref}", str(e)
        ) from e


def replace_image(
    doc: fitz.Document,
    img_xref: int,
    page_number: int,
    temp_image_path: str,
    grayscale: bool,
) -> None:
    """
    Replace a raster image on a PDF page with an optimized image.

    Args:
        doc (fitz.Document): The PDF document.
        img_xref (int): The image reference.
        page_number (int): The page number.
        temp_image_path (str): The path to the temporary image file.
        grayscale (bool): Whether to convert the image to grayscale.

    Raises:
        PDFProcessingError: If image replacement fails.
    """
    try:
        page: fitz.Page = doc[page_number]
        base_image: fitz.Pixmap = fitz.Pixmap(doc, img_xref)

        if grayscale and base_image.n > 2:
            base_image = fitz.Pixmap(fitz.csGRAY, base_image)

        img_rect: Optional[fitz.Rect] = get_image_rect(page, img_xref)
        if img_rect is None:
            raise PDFProcessingError(
                f"Failed to get image rectangle for image {img_xref}",
                "Image not found on page",
            )

        image_bytes: bytes = base_image.tobytes("png")

        try:
            logging.debug(
                "Original image size for image %d: %d bytes",
                {img_xref},
                {len(image_bytes)},
            )
            optimized_image_bytes: bytes = optimize_raster_image(image_bytes)
            logging.debug(
                "Optimized image size for image %d: %d bytes",
                {img_xref},
                {len(optimized_image_bytes)},
            )

            if len(optimized_image_bytes) >= len(image_bytes):
                logging.info(
                    "Original image with xref = %d was already highly optimized. "
                    "Keeping original.",
                    img_xref,
                )
                return

            replace_image_with_optimized_version(
                page, optimized_image_bytes, img_rect, img_xref
            )
        except Exception as e:
            raise ImageProcessingError(
                "Failed during fallback image replacement", str(e)
            ) from e

        page.delete_image(img_xref)  # type: ignore  # pymupdf library does not expose this method
        logging.info(
            "Successfully replaced image %d on page %d with an optimized version.",
            img_xref,
            page_number,
        )
    except Exception as e:
        raise PDFProcessingError(
            f"Unexpected error while replacing image {img_xref} on page {page_number}",
            f"Error: {str(e)}\nTraceback: {traceback.format_exc()}",
        ) from e
    finally:
        if temp_image_path and os.path.exists(temp_image_path):
            try:
                os.remove(temp_image_path)
            except Exception as e:  # pylint: disable=broad-except
                logging.warning(
                    "Failed to remove temporary file %s: %s",
                    temp_image_path,
                    str(e),
                )


def _log_process_image_pdf_processing_error(
    page_num: int, e: PDFProcessingError, xref: int
) -> None:
    """
    Log a PDFProcessingError that occurred while processing an image.

    Args:
        page_num (int): The page number where the error occurred.
        e (PDFProcessingError): The error that occurred.
        xref (int): The image reference.
    """
    logging.debug(
        "PDFProcessingError while processing image from page %d: %s",
        page_num,
        str(e),
    )
    logging.debug("Error: %s", str(e))
    logging.debug("Image xref: %i", xref)
    logging.debug("Cause: %s", str(e.__cause__))
    logging.debug("Context: %s", e.context)
    logging.info("Keeping the original image %d.", xref)


def process_image(args: Tuple[fitz.Document, int, int, str, bool]) -> None:
    """
    Process a single image in a PDF document.

    Args:
        args (Tuple[fitz.Document, int, int, str, bool]): A tuple containing:
            - The PDF document
            - The image reference
            - The page number
            - The path to the temporary image file
            - A boolean indicating whether to convert to grayscale
    """
    doc, xref, page_num, image_path, grayscale = args
    try:
        replace_image(doc, xref, page_num, image_path, grayscale)
    except PDFProcessingError as e:
        _log_process_image_pdf_processing_error(page_num, e, xref)
    except Exception as e:  # pylint: disable=broad-except
        logging.error(
            "Unexpected error while processing image from page %d: %s",
            page_num,
            str(e),
        )
        logging.debug("Traceback: %s", traceback.format_exc())
        logging.info("Keeping original image %d", xref)


def process_images(
    doc: fitz.Document, num_threads: int, grayscale: bool
) -> None:
    """
    Process all images in a PDF document using multiple threads.

    Args:
        doc (fitz.Document): The PDF document.
        num_threads (int): The number of threads to use for processing.
        grayscale (bool): Whether to convert images to grayscale.

    Raises:
        PDFProcessingError: If an error occurs during image processing.
    """
    image_tasks: List[Tuple[fitz.Document, int, int, str, bool]] = []

    try:
        # pylint: disable=consider-using-enumerate  # fitz.Document is not enumerable
        for page_num in range(len(doc)):
            page: fitz.Page = doc[page_num]
            for img in page.get_images(full=True):
                img_xref = img[0]
                pixmap = fitz.Pixmap(doc, img_xref)
                temp_file = create_temp_file(suffix=".png")
                pixmap.save(temp_file.name)
                image_tasks.append(
                    (
                        doc,
                        int(img_xref),
                        page_num,
                        str(temp_file.name),
                        grayscale,
                    )
                )
        # pylint: enable=consider-using-enumerate
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=num_threads
        ) as executor:
            list(executor.map(process_image, image_tasks))
    except Exception as e:
        raise PDFProcessingError(
            "Error processing images", traceback.format_exc()
        ) from e


## Vector Path Simplification


def simplify_vector_path(
    points: List[Tuple[float, float]],
    epsilon: float,
    aggressiveness: float = 1.0,
) -> List[Tuple[float, float]]:
    """
    Aggressively simplify a vector path by reducing the number of points.

    Args:
        points (List[Tuple[float, float]]): The original path points.
        epsilon (float): The simplification threshold.
        aggressiveness (float): Factor to increase simplification aggressiveness.

    Returns:
        List[Tuple[float, float]]: The simplified path points.
    """

    def point_line_distance(point, start, end) -> float:
        if start == end:
            return math.hypot(point[0] - start[0], point[1] - start[1])
        n: float = abs(
            (end[0] - start[0]) * (start[1] - point[1])
            - (start[0] - point[0]) * (end[1] - start[1])
        )
        d: float = math.hypot(end[0] - start[0], end[1] - start[1])
        return n / d

    if len(points) < 3:
        return points

    simplified: List[Tuple[float, float]] = [points[0]]
    for i in range(1, len(points) - 1):
        if (
            point_line_distance(points[i], simplified[-1], points[-1])
            > epsilon
        ):
            simplified.append(points[i])
    simplified.append(points[-1])

    return simplified


def extreme_simplify_vector_path(
    points: List[Tuple[float, float]],
    epsilon: float,
    aggressiveness: float = 1.0,
) -> List[Tuple[float, float]]:
    """
    Extremely aggressively simplify a vector path by drastically reducing the number of points.

    Args:
        points (List[Tuple[float, float]]): The original path points.
        epsilon (float): The simplification threshold.
        aggressiveness (float): Factor to increase simplification aggressiveness.

    Returns:
        List[Tuple[float, float]]: The extremely simplified path points.
    """
    if len(points) < 3:
        return points

    effective_epsilon: float = epsilon * (aggressiveness**2)

    def point_line_distance(point, start, end) -> float:
        if start == end:
            return math.hypot(point[0] - start[0], point[1] - start[1])
        n: float = abs(
            (end[0] - start[0]) * (start[1] - point[1])
            - (start[0] - point[0]) * (end[1] - start[1])
        )
        d: float = math.hypot(end[0] - start[0], end[1] - start[1])
        return n / d

    simplified: List[Tuple[float, float]] = [points[0]]
    for i in range(1, len(points) - 1):
        if (
            point_line_distance(points[i], simplified[-1], points[-1])
            > effective_epsilon
        ):
            simplified.append(points[i])
    simplified.append(points[-1])

    # Additional aggressive simplification
    while len(simplified) > 3 and len(simplified) > len(points) / 5:
        temp: List[Tuple[float, float]] = [simplified[0]]
        for i in range(1, len(simplified) - 1, 2):
            if (
                point_line_distance(simplified[i], temp[-1], simplified[-1])
                > effective_epsilon
            ):
                temp.append(simplified[i])
        temp.append(simplified[-1])
        if len(temp) == len(simplified):
            break
        simplified = temp

    return simplified


def extreme_vector_optimization(
    page: fitz.Page, epsilon: float, aggressiveness: float = 1.0
) -> Tuple[int, int]:
    """
    Perform extreme vector path optimization on a PDF page.

    Args:
        page (fitz.Page): The PDF page to optimize.
        epsilon (float): The simplification threshold.
        aggressiveness (float): Factor to increase simplification aggressiveness.

    Returns:
        Tuple[int, int]: The total number of points before and
                         after optimization.

    Raises:
        VectorOptimizationError: If an error occurs during vector optimization.
    """
    try:
        shape: Any = page.new_shape()  # type: ignore  # fitz methods don't support type checking
        total_points_before: int = 0
        total_points_after: int = 0
        for path in page.get_drawings():
            if path["type"] in ["l", "c"]:  # line or curve
                points: List[Tuple[float, float]] = path["pts"]
                total_points_before += len(points)
                simplified_points: List[Tuple[float, float]] = (
                    simplify_vector_path(points, epsilon, aggressiveness)
                )
                total_points_after += len(simplified_points)
                if path["type"] == "l" and len(simplified_points) >= 2:
                    shape.draw_polyline(simplified_points)
                elif path["type"] == "c" and len(simplified_points) >= 4:
                    shape.draw_bezier(simplified_points)
            elif path["type"] in ["f", "s"]:  # fill or stroke
                shape.finish(fill=path["type"] == "f", color=path.get("color"))
            elif path["type"] == "e":  # ellipse
                rect: fitz.Rect = fitz.Rect(path["rect"])
                shape.draw_oval(rect)

        shape.commit()
        return total_points_before, total_points_after
    except Exception as e:
        raise VectorOptimizationError(
            f"Error during extreme vector optimization: {str(e)}",
            traceback.format_exc(),
        ) from e


def shape_aware_extreme_optimization(
    page: fitz.Page, epsilon: float, aggressiveness: float = 1.0
) -> Tuple[int, int]:
    """
    Perform shape-aware extreme vector path optimization on a PDF page.

    Args:
        page (fitz.Page): The PDF page to optimize.
        epsilon (float): The simplification threshold.
        aggressiveness (float): Factor to increase simplification aggressiveness.

    Returns:
        Tuple[int, int]: The total number of points before and after optimization.
    """

    def is_rectangular(
        points: List[Tuple[float, float]], tolerance: float = 0.1
    ) -> bool:
        if (
            len(points) != 5
        ):  # A rectangle should have 5 points (including the repeated first point)
            return False

        # Check if opposite sides are parallel and equal in length
        def are_parallel_and_equal(side1, side2, tolerance) -> bool:
            len1: float = math.hypot(side1[0], side1[1])
            len2: float = math.hypot(side2[0], side2[1])
            return abs(
                side1[0] * side2[1] - side1[1] * side2[0]
            ) < tolerance and abs(len1 - len2) < tolerance * max(len1, len2)

        side1: Tuple[float, float] = (
            points[1][0] - points[0][0],
            points[1][1] - points[0][1],
        )
        side2: Tuple[float, float] = (
            points[2][0] - points[1][0],
            points[2][1] - points[1][1],
        )
        side3: Tuple[float, float] = (
            points[3][0] - points[2][0],
            points[3][1] - points[2][1],
        )
        side4: Tuple[float, float] = (
            points[0][0] - points[3][0],
            points[0][1] - points[3][1],
        )

        return are_parallel_and_equal(
            side1, side3, tolerance
        ) and are_parallel_and_equal(side2, side4, tolerance)

    def is_circular(
        points: List[Tuple[float, float]], tolerance: float = 0.1
    ) -> bool:
        if len(points) < 8:  # Arbitrary minimum number of points for a circle
            return False

        # Calculate center and average radius
        cx: float = sum(p[0] for p in points) / len(points)
        cy: float = sum(p[1] for p in points) / len(points)
        avg_radius: float = sum(
            math.hypot(p[0] - cx, p[1] - cy) for p in points
        ) / len(points)

        # Check if all points are approximately on the circle
        return all(
            abs(math.hypot(p[0] - cx, p[1] - cy) - avg_radius)
            < tolerance * avg_radius
            for p in points
        )

    try:
        shape: Any = page.new_shape()  # type: ignore  # fitz methods don't support type checking
        total_points_before: int = 0
        total_points_after: int = 0

        for path in page.get_drawings():
            if path["type"] in ["l", "c"]:  # line or curve
                points: List[Tuple[float, float]] = path["pts"]
                total_points_before += len(points)

                if is_rectangular(points):
                    # Simplify to exact rectangle
                    simplified_points = [
                        points[0],
                        points[1],
                        points[2],
                        points[3],
                        points[0],
                    ]
                elif is_circular(points):
                    # Simplify to circle/ellipse
                    rect = fitz.Rect(points[0], points[-1]).normalize()
                    shape.draw_oval(rect)
                    simplified_points: List[Tuple[float, float]] = []
                else:
                    # Extreme simplification for other shapes
                    simplified_points: List[Tuple[float, float]] = (
                        extreme_simplify_vector_path(
                            points, epsilon, aggressiveness
                        )
                    )

                total_points_after += len(simplified_points)

                if simplified_points:
                    if path["type"] == "l":
                        shape.draw_polyline(simplified_points)
                    elif path["type"] == "c" and len(simplified_points) >= 4:
                        shape.draw_bezier(simplified_points)
            elif path["type"] in ["f", "s"]:  # fill or stroke
                shape.finish(fill=path["type"] == "f", color=path.get("color"))
            elif path["type"] == "e":  # ellipse
                rect: fitz.Rect = fitz.Rect(path["rect"])
                shape.draw_oval(rect)

        shape.commit()
        return total_points_before, total_points_after
    except Exception as e:
        raise VectorOptimizationError(
            f"Error during shape-aware extreme vector optimization: {str(e)}",
            traceback.format_exc(),
        ) from e


def calculate_ei(
    mu: np.ndarray, mu_sample_opt: float, xi: float, sigma: np.ndarray
) -> np.ndarray:
    """
    Calculate the expected improvement values for Bayesian optimization.

    Args:
        mu (np.ndarray): The mean predictions.
        mu_sample_opt (float): The current best observed value.
        xi (float): Exploration-exploitation trade-off parameter.
        sigma (np.ndarray): The standard deviations of the predictions.

    Returns:
        np.ndarray: The expected improvement values.
    """
    imp = mu - mu_sample_opt - xi
    z = np.divide(imp, sigma, out=np.zeros_like(sigma), where=sigma != 0)
    ei = imp * norm.cdf(z) + sigma * norm.pdf(z)
    ei[sigma == 0.0] = 0.0
    return ei


def expected_improvement(
    x: np.ndarray,
    x_sample: np.ndarray,
    y_sample: np.ndarray,
    gpr: GaussianProcessRegressor,
    xi: float = 0.01,
) -> np.ndarray:
    """
    Calculate the expected improvement for Bayesian optimization.

    Args:
        x (np.ndarray): The input points.
        x_sample (np.ndarray): The sample points.
        y_sample (np.ndarray): The sample values.
        gpr (GaussianProcessRegressor): The Gaussian Process Regressor.
        xi (float, optional): Exploration-exploitation trade-off parameter.
                              Defaults to 0.01.

    Returns:
        np.ndarray: The expected improvement values.
    """
    mu, sigma = gpr.predict(x.reshape(-1, 2), return_std=True)  # type: ignore
    mu_sample = gpr.predict(x_sample)

    sigma = sigma.reshape(-1, 1)
    mu_sample_opt = np.max(mu_sample)

    with np.errstate(divide="warn"):
        imp = mu - mu_sample_opt - xi
        z = np.divide(imp, sigma, out=np.zeros_like(sigma), where=sigma != 0)
        ei = imp * norm.cdf(z) + sigma * norm.pdf(z)
        ei[sigma == 0.0] = 0.0

    return ei.ravel()


def propose_location(
    acquisition: Callable[..., np.ndarray],
    x_sample: np.ndarray,
    y_sample: np.ndarray,
    gpr: GaussianProcessRegressor,
    bounds: np.ndarray,
    n_restarts: int = 25,
) -> Optional[np.ndarray]:
    """
    Propose the next sample location for Bayesian optimization.

    Args:
        acquisition (Callable[..., np.ndarray]): The acquisition function.
        x_sample (np.ndarray): The sample points.
        y_sample (np.ndarray): The sample values.
        gpr (GaussianProcessRegressor): The Gaussian Process Regressor.
        bounds (np.ndarray): The bounds for optimization.
        n_restarts (int, optional): The number of optimization restarts.
                                    Defaults to 25.

    Returns:
        Optional[np.ndarray]: The proposed location, or None
                              if optimization fails.
    """
    min_val: float = float("inf")
    min_x: Optional[np.ndarray] = None

    def min_obj(x: np.ndarray) -> np.ndarray:
        return -acquisition(x.reshape(1, -1), x_sample, y_sample, gpr).ravel()

    for x0 in np.random.uniform(
        bounds[:, 0], bounds[:, 1], size=(n_restarts, 2)
    ):
        res = minimize(min_obj, x0=x0, bounds=bounds, method="L-BFGS-B")
        if res.fun < min_val:
            min_val = (
                res.fun[0] if isinstance(res.fun, np.ndarray) else res.fun
            )
            min_x = res.x
    return min_x


def bayesian_optimization(
    objective: Callable[[np.ndarray], float],
    bounds: np.ndarray,
    n_iterations: int,
    n_initial: int = 5,
) -> Tuple[float, float]:
    """
    Perform Bayesian optimization to find the optimal value for a given
    objective function.

    Args:
        objective (Callable[[np.ndarray], float]): The objective function to optimize.
        bounds (np.ndarray): The bounds for optimization.
        n_iterations (int): The number of optimization iterations.
        n_initial (int, optional): The number of initial random samples.
                                   Defaults to 5.

    Returns:
        Tuple[float, float]: The best input values (epsilon, aggressiveness)
                             for vector simplification.
    """
    dim: int = bounds.shape[0]
    x_sample: np.ndarray = np.random.uniform(
        bounds[:, 0], bounds[:, 1], size=(n_initial, dim)
    )
    y_sample: np.ndarray = np.array([objective(x) for x in x_sample])

    kernel: Sum = Matern(
        nu=2.5, length_scale=[1.0] * dim, length_scale_bounds=(1e-2, 1e5)
    ) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-12, 1e2))
    gpr: GaussianProcessRegressor = GaussianProcessRegressor(
        kernel=kernel, n_restarts_optimizer=25, random_state=42
    )

    for i in range(n_iterations):
        gpr.fit(x_sample, y_sample)

        next_sample: Optional[np.ndarray] = propose_location(
            expected_improvement, x_sample, y_sample, gpr, bounds
        )

        if next_sample is None:
            logging.warning(
                "Optimization stopped at iteration %d due to "
                "failure in proposing next sample.",
                (i + 1),
            )
            break

        next_objective: float = objective(next_sample)
        logging.debug(
            "Iteration %d/%d: epsilon = %.4f, aggressiveness = %.4f, objective = %.4f",
            i + 1,
            n_iterations,
            next_sample[0],
            next_sample[1],
            next_objective,
        )

        x_sample = np.vstack((x_sample, next_sample))
        y_sample = np.append(y_sample, next_objective)

    best_idx: int = int(np.argmax(y_sample))
    best_params: np.ndarray = x_sample[best_idx]

    return float(best_params[0]), float(best_params[1])


def text_aware_vector_optimization(
    page: fitz.Page, epsilon: float, aggressiveness: float = 1.0
) -> Tuple[int, int]:
    """
    Perform text-aware vector path optimization on a PDF page.

    Args:
        page (fitz.Page): The PDF page to optimize.
        epsilon (float): The simplification threshold.
        aggressiveness (float): Factor to increase simplification aggressiveness.

    Returns:
        Tuple[int, int]: The total number of points before and after optimization.
    """
    try:

        def simplify_path(points, epsilon, is_text_area):
            if len(points) < 3:
                return points

            if is_text_area:
                # Use less aggressive simplification for text areas
                return simplify_vector_path(points, epsilon)
            else:
                # Use more aggressive simplification for non-text areas
                return extreme_simplify_vector_path(
                    points, epsilon, aggressiveness
                )

        shape = page.new_shape()  # type: ignore  # fitz methods don't support type checking
        total_points_before = 0
        total_points_after = 0

        # Get text areas
        text_areas = [word[:4] for word in page.get_text("words")]  # type: ignore  # fitz methods don't support type checking

        for path in page.get_drawings():
            if path["type"] in ["l", "c"]:  # line or curve
                points = path["pts"]
                total_points_before += len(points)

                # Check if path intersects with any text area
                path_rect = fitz.Rect(points[0], points[-1]).normalize()
                intersects_text = any(
                    path_rect.intersects(fitz.Rect(area))
                    for area in text_areas
                )

                simplified_points = simplify_path(
                    points, epsilon, intersects_text
                )
                total_points_after += len(simplified_points)

                if path["type"] == "l" and len(simplified_points) >= 2:
                    shape.draw_polyline(simplified_points)
                elif path["type"] == "c" and len(simplified_points) >= 4:
                    shape.draw_bezier(simplified_points)
            elif path["type"] in ["f", "s"]:  # fill or stroke
                shape.finish(fill=path["type"] == "f", color=path.get("color"))
            elif path["type"] == "e":  # ellipse
                rect = fitz.Rect(path["rect"])
                shape.draw_oval(rect)

        shape.commit()
        return total_points_before, total_points_after
    except Exception as e:
        raise VectorOptimizationError(
            f"Error during text-aware vector optimization: {str(e)}",
            traceback.format_exc(),
        ) from e


def find_optimal_vector_simplification(
    doc: fitz.Document,
    min_epsilon: float = 0.01,
    max_epsilon: float = 10.0,
    min_aggressiveness: float = 1.0,
    max_aggressiveness: float = 10.0,
    n_iterations: int = 40,
    sample_size: int = 5,
    similarity_threshold: float = 0.5,
) -> Tuple[float, float]:
    """
    Find the optimal vector simplification epsilon value and aggressiveness
    factor for extreme simplification using Bayesian optimization.

    Args:
        doc (fitz.Document): The PDF document.
        min_epsilon (float): The minimum epsilon value.
        max_epsilon (float): The maximum epsilon value.
        min_aggressiveness (float): The minimum aggressiveness factor.
        max_aggressiveness (float): The maximum aggressiveness factor.
        n_iterations (int): The number of optimization iterations.
        sample_size (int): The number of pages to sample.
        similarity_threshold (float): The similarity threshold.

    Returns:
        Tuple[float, float]: The optimal epsilon value and aggressiveness factor
                             for vector simplification.
    """
    try:
        num_pages: int = len(doc)
        sample_pages: List[int] = sorted(
            random.sample(range(num_pages), min(sample_size, num_pages))
        )

        def objective(params: np.ndarray) -> float:
            epsilon, aggressiveness = params
            total_points_before: int = 0
            total_points_after: int = 0

            for page_num in sample_pages:
                page = doc[page_num]
                points_before, points_after = shape_aware_extreme_optimization(
                    page, epsilon, aggressiveness
                )
                total_points_before += points_before
                total_points_after += points_after

            similarity: float = (
                total_points_after / total_points_before
                if total_points_before > 0
                else 1.0
            )
            simplification_score: float = (
                1 - similarity
            ) ** 2  # Squared to favor more extreme simplification
            readability_penalty: float = (
                max(0, similarity_threshold - similarity) * 2
            )

            return simplification_score - readability_penalty

        bounds: np.ndarray = np.array(
            [
                [min_epsilon, max_epsilon],
                [min_aggressiveness, max_aggressiveness],
            ]
        )
        result: OptimizeResult = minimize(
            lambda x: -objective(x),
            x0=np.array(
                [
                    (min_epsilon + max_epsilon) / 2,
                    (min_aggressiveness + max_aggressiveness) / 2,
                ]
            ),
            bounds=bounds,
            method="L-BFGS-B",
            options={"maxiter": n_iterations},
        )

        best_epsilon, best_aggressiveness = result.x
        best_score: float = -result.fun

        logging.info(
            "Extreme optimization complete. Best epsilon: %.4f, Best aggressiveness: %.4f, Best score: %.4f",
            best_epsilon,
            best_aggressiveness,
            best_score,
        )

        return float(best_epsilon), float(best_aggressiveness)
    except Exception as e:
        raise VectorOptimizationError(
            "Error in find_optimal_vector_simplification",
            traceback.format_exc(),
        ) from e


## Fonts optimization


def get_used_characters(doc: fitz.Document) -> Set[str]:
    """
    Get the set of used characters from a PDF document.

    Args:
        doc (fitz.Document): The PDF document.

    Returns:
        Set[str]: The set of used characters.

    Raises:
        PDFProcessingError: If character extraction fails.
    """
    try:
        used_chars: Set[str] = set()
        for page in doc:
            text: str = page.get_text()  # type: ignore  # pymupdf library does not expose this method
            used_chars.update(set(text))
        return used_chars
    except Exception as e:
        raise PDFProcessingError(
            "Failed to extract used characters", str(e)
        ) from e


def aggressive_font_subset(pdf: pikepdf.Pdf, used_chars: Set[str]) -> None:
    """
    Perform aggressive font subsetting on a PDF object based on the
    used characters.

    Args:
        pdf (pikepdf.Pdf): The PDF object.
        used_chars (Set[str]): The set of used characters.

    Raises:
        FontSubsettingError: If font subsetting fails.
    """
    for page in pdf.pages:
        if "/Resources" in page and "/Font" in page["/Resources"]:
            fonts: pikepdf.Object = page["/Resources"]["/Font"]
            for _font_key, font in fonts.items():
                if "/FontDescriptor" in font:
                    descriptor: pikepdf.Object = font["/FontDescriptor"]
                    for key in ["/FontFile", "/FontFile2", "/FontFile3"]:
                        if key in descriptor:
                            font_file: pikepdf.Object = descriptor[key]
                            if isinstance(font_file, pikepdf.Stream):
                                font_data: bytes = font_file.read_bytes()
                                try:
                                    ttfont: TTFont = TTFont(
                                        io.BytesIO(font_data)
                                    )
                                    subsetter: subset.Subsetter = (
                                        subset.Subsetter()
                                    )
                                    subsetter.populate(
                                        text="".join(used_chars)
                                    )
                                    subsetter.subset(ttfont)
                                    new_font_buffer: io.BytesIO = io.BytesIO()
                                    ttfont.save(new_font_buffer)
                                    new_font_data: bytes = (
                                        new_font_buffer.getvalue()
                                    )
                                    descriptor[key] = pikepdf.Stream(
                                        pdf, new_font_data
                                    )
                                except Exception as e:
                                    raise FontSubsettingError(
                                        f"Failed to subset font: {str(e)}",
                                        traceback.format_exc(),
                                    ) from e


## Metadata optimization


def remove_metadata(pdf: pikepdf.Pdf) -> None:
    """
    Remove metadata from a PDF object.

    Args:
        pdf (pikepdf.Pdf): The PDF object.
    """
    with pdf.open_metadata() as meta:
        meta.clear()
    if pdf.Root.get("/Metadata") is not None:
        del pdf.Root["/Metadata"]


## Objects optimization


def clean_page_resources(page, pdf) -> None:
    """
    Removes duplicated and unnecessary indirect objects from page resources.

    Args:
        page (pikepdf.Object): A page in the PDF.
        seen_objects (dict): A dictionary to track already seen objects.
    """
    if "/Resources" not in page:
        return
    resources = page["/Resources"]
    for resource_type in ["/XObject", "/ExtGState", "/Pattern", "/Shading"]:
        if resource_type in resources:
            resource_dict = resources[resource_type]
            seen_objects = {}
            keys_to_delete = []

            for key, value in resource_dict.items():
                if value.is_indirect:
                    object_id = value.objgen[0]
                    if object_id in seen_objects:
                        # Replace duplicate with a reference to the first seen
                        resources[resource_type][key] = seen_objects[object_id]
                        continue
                    seen_objects[object_id] = value
                    # Check if the object is necessary
                    try:
                        _ = pdf.get_object(value.objgen)
                    except KeyError:
                        keys_to_delete.append(key)

            for key in keys_to_delete:
                del resource_dict[key]


## Annotations optimization


def flatten_annotations(doc: fitz.Document) -> None:
    """
    Flatten annotations in a PDF document.

    Args:
        doc (fitz.Document): The PDF document.

    Raises:
        PDFProcessingError: If annotation flattening fails.
    """
    try:
        for page in doc:
            for annot in page.annots():
                if annot.type[0] in [
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    12,
                    13,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19,
                    20,
                ]:  # Form fields and annotations
                    page.add_redact_annot(annot.rect)
                    page.apply_redactions()  # type: ignore  # pymupdf library does not expose this method
    except Exception as e:
        raise PDFProcessingError(
            "Failed to flatten annotations", traceback.format_exc()
        ) from e


## Content stream optimization


@lru_cache(maxsize=1000)
def optimize_content_stream(content: bytes) -> bytes:
    """
    Optimize a content stream by removing empty lines and
    lines starting with '%'.

    Args:
        content (bytes): The content stream.

    Returns:
        bytes: The optimized content stream.
    """
    return b"\n".join(
        line
        for line in content.split(b"\n")
        if line.strip() and not line.strip().startswith(b"%")
    )


def process_content(content: bytes) -> bytes:
    """
    Optimize the content streams of a PDF.

    Args:
        content (bytes): The content stream.

    Returns:
        bytes: The optimized content stream.

    Raises:
        ContentOptimizationError: If content optimization fails.
    """
    try:
        return optimize_content_stream(content)
    except Exception as e:
        raise ContentOptimizationError(
            f"Failed to optimize content stream: {str(e)}",
            traceback.format_exc(),
        ) from e


def optimize_pdf_contents(pdf: pikepdf.Pdf) -> None:
    """
    Optimize the content streams of a PDF file.

    Args:
        pdf (pikepdf.Pdf): The PDF object.

    Raises:
        ContentOptimizationError: If content optimization fails.
    """
    with multiprocessing.Pool() as pool:
        for page in pdf.pages:
            if "/Contents" in page:
                contents: pikepdf.Object = page["/Contents"]
                try:
                    if isinstance(contents, pikepdf.Stream):
                        optimized: ApplyResult = pool.apply_async(
                            process_content, (contents.read_bytes(),)
                        )
                        page["/Contents"] = pikepdf.Stream(
                            pdf, optimized.get()
                        )
                    elif isinstance(contents, pikepdf.Array):
                        optimized_contents: List[pikepdf.Stream] = []
                        for content in contents:  # type: ignore
                            if isinstance(content, pikepdf.Stream):
                                optimized = pool.apply_async(
                                    process_content, (content.read_bytes(),)
                                )
                                optimized_contents.append(
                                    pikepdf.Stream(pdf, optimized.get())
                                )
                        page["/Contents"] = pikepdf.Array(optimized_contents)
                except Exception as e:
                    raise ContentOptimizationError(
                        f"Failed to optimize content stream: {str(e)}",
                        traceback.format_exc(),
                    ) from e


# PDF Processing


def process_single_pdf(
    args: Tuple[str, str, int, bool, bool]
) -> Tuple[str, int, int, float]:
    """
    Process a single PDF file with retry mechanism and adaptive concurrency.

    Args:
        args (Tuple[str, str, int, bool, bool]): Input file, output file, num_threads,
                                                 grayscale, flatten.

    Returns:
        Tuple[str, int, int, float]: File name, original size, optimized size,
                                     and reduction percentage.
    """
    input_file, output_file, num_threads, grayscale, flatten = args
    for attempt in range(MAX_RETRIES):
        try:
            update_semaphore_value()  # Update semaphore before processing
            original_size, optimized_size, reduction_percentage = (
                extreme_pdf_optimization(
                    input_file, output_file, num_threads, grayscale, flatten
                )
            )
            return (
                os.path.basename(input_file),
                original_size,
                optimized_size,
                reduction_percentage,
            )
        except OSError as e:
            if attempt < MAX_RETRIES - 1:
                logging.warning(
                    "Attempt %d failed for %s: %s. Retrying...",
                    attempt + 1,
                    input_file,
                    str(e),
                )
                time.sleep(RETRY_DELAY)
            else:
                logging.error(
                    "Failed to process %s after %d attempts: %s",
                    input_file,
                    MAX_RETRIES,
                    str(e),
                )
                return os.path.basename(input_file), 0, 0, 0.0
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Error processing %s: %s", input_file, str(e))
            logging.error(traceback.format_exc())
            return os.path.basename(input_file), 0, 0, 0.0
    return os.path.basename(input_file), 0, 0, 0.0


def optimize_directory(
    input_dir: str,
    output_dir: str,
    num_threads: int,
    grayscale: bool,
    flatten: bool,
) -> None:
    """
    Optimize all PDF files in the given input directory and save them
    to the output directory.

    Args:
        input_dir (str): The path to the input directory containing PDF files.
        output_dir (str): The path to save the optimized PDF files.
        num_threads (int): The number of threads to use for processing.
        grayscale (bool): Whether to convert images to grayscale.
        flatten (bool): Whether to flatten form fields and annotations.

    Raises:
        FileAccessError: If the input directory is not found or cannot be accessed.
        PDFProcessingError: If PDF processing fails.
    """
    if not os.path.exists(input_dir):
        raise FileAccessError(f"Input directory not found: {input_dir}", "")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pdf_files: List[str] = [
        f for f in os.listdir(input_dir) if f.lower().endswith(".pdf")
    ]
    total_files: int = len(pdf_files)

    logging.info("Found %d PDF files to process.", total_files)

    # Prepare arguments for parallel processing
    process_args: List[Tuple] = [
        (
            os.path.join(input_dir, pdf_file),
            os.path.join(output_dir, f"optimized_{pdf_file}"),
            max(
                1, num_threads // total_files
            ),  # Distribute threads among files
            grayscale,
            flatten,
        )
        for pdf_file in pdf_files
    ]

    # Use ThreadPoolExecutor for parallel processing
    with concurrent.futures.ThreadPoolExecutor(
        max_workers=file_semaphore.max_value
    ) as executor:
        futures: List[
            concurrent.futures.Future[Tuple[str, int, int, float]]
        ] = [
            executor.submit(process_single_pdf, args) for args in process_args
        ]

        for future in concurrent.futures.as_completed(futures):
            file_name, original_size, optimized_size, reduction_percentage = (
                future.result()
            )
            if original_size > 0:
                logging.info("Processed: %s", file_name)
                logging.info("Original size: %d bytes", original_size)
                logging.info("Optimized size: %d bytes", optimized_size)
                logging.info("Size reduction: %.2f%%", reduction_percentage)
            else:
                logging.error("Failed to process: %s", file_name)

            # Log current semaphore value and open files count
            logging.info(
                "Current semaphore value: %d",
                file_semaphore.get_current_value(),
            )
            logging.info(
                "Current open files count: %d", get_open_files_count()
            )


@with_adaptive_semaphore
def extreme_pdf_optimization(
    input_file: str,
    output_file: str,
    num_threads: int,
    grayscale: bool,
    flatten: bool,
    aggressive_mode: bool = True,
) -> Tuple[int, int, float]:
    """
    Perform extreme optimization on a PDF file.

    Args:
        input_file (str): The path to the input PDF file.
        output_file (str): The path to save the optimized PDF file.
        num_threads (int): The number of threads to use for processing.
        grayscale (bool): Whether to convert images to grayscale.
        flatten (bool): Whether to flatten form fields and annotations.

    Returns:
        Tuple[int, int, float]: Original size, optimized size, and reduction percentage.

    Raises:
        FileAccessError: If the input file is not found or cannot be accessed.
        PDFProcessingError: If PDF processing fails.
    """
    temp_files: List[str] = []

    try:
        logging.info("Opening PDF: %s", input_file)

        if not os.path.exists(input_file):
            raise FileAccessError(f"Input file not found: {input_file}", "")

        temp_pdf_path: str = create_temp_file(suffix=".pdf").name
        temp_files.append(temp_pdf_path)

        shutil.copy2(input_file, temp_pdf_path)

        pdf: pikepdf.Pdf = pikepdf.open(temp_pdf_path)
        doc: fitz.Document = fitz.open(temp_pdf_path)

        logging.info("Removing unreferenced resources...")
        pdf.remove_unreferenced_resources()

        # Clean PDF and remove hidden objects
        logging.info("Cleaning duplicated and unnecessary indirect objects...")
        for page_num in range(len(doc)):
            page: fitz.Page = doc[page_num]
            try:
                # remove_hidden_objects(page, pdf)
                if "/Resources" in pdf.pages[page_num]:
                    clean_page_resources(pdf.pages[page_num], pdf)
            except Exception as e:
                logging.warning(f"Error processing page {page_num}: {str(e)}")

        # Define the objective function for Bayesian optimization
        def optimization_objective(params) -> float:
            epsilon, aggressiveness = params
            total_reduction = 0
            total_quality_loss = 0

            for page in doc:
                original_size: int = len(page.get_contents())
                if aggressive_mode:
                    _, points_after = text_aware_vector_optimization(
                        page, epsilon, aggressiveness
                    )
                else:
                    _, points_after = shape_aware_extreme_optimization(
                        page, epsilon, aggressiveness
                    )
                optimized_size: int = len(page.get_contents())

                size_reduction: float = (
                    original_size - optimized_size
                ) / original_size
                total_reduction += size_reduction

                # Calculate quality loss (this is a simplified measure and might need refinement)
                quality_loss: float = 1 - (points_after / original_size)
                total_quality_loss += quality_loss

            avg_reduction: float = total_reduction / len(doc)
            avg_quality_loss: float = total_quality_loss / len(doc)

            # Balance between size reduction and quality preservation
            return avg_reduction - (0.5 * avg_quality_loss)

        # Perform Bayesian optimization
        logging.info("Finding optimal parameters for vector simplification...")
        bounds: np.ndarray[Any, np.dtype[Any]] = np.array(
            [[0.01, 10.0], [1.0, 10.0]]
        )  # for epsilon and aggressiveness
        optimal_params: Tuple[float, float] = bayesian_optimization(
            optimization_objective, bounds, n_iterations=20
        )
        optimal_epsilon, optimal_aggressiveness = optimal_params

        logging.info(
            "Optimal parameters: epsilon=%.4f, aggressiveness=%.4f",
            optimal_epsilon,
            optimal_aggressiveness,
        )

        # Apply vector optimizations
        logging.info("Applying vector optimizations...")
        for page in doc:
            if aggressive_mode:
                text_aware_vector_optimization(
                    page, optimal_epsilon, optimal_aggressiveness
                )
            else:
                shape_aware_extreme_optimization(
                    page, optimal_epsilon, optimal_aggressiveness
                )

        logging.info("Collecting used characters...")
        used_chars: Set[str] = get_used_characters(doc)
        doc.close()

        logging.info("Aggressively subsetting fonts...")
        aggressive_font_subset(pdf, used_chars)

        logging.info("Removing metadata...")
        remove_metadata(pdf)

        logging.info("Optimizing content streams...")
        optimize_pdf_contents(pdf)

        temp_pdf_path2: str = create_temp_file(suffix=".pdf").name
        temp_files.append(temp_pdf_path2)

        pdf.save(temp_pdf_path2)
        pdf.close()

        doc: fitz.Document = fitz.open(temp_pdf_path2)

        if flatten:
            logging.info("Flattening annotations...")
            flatten_annotations(doc)

        logging.info(
            "Finding optimal extreme vector simplification parameters..."
        )
        optimal_epsilon, optimal_aggressiveness = (
            find_optimal_vector_simplification(doc)
        )

        logging.info(
            "Optimal extreme vector simplification epsilon: %f, aggressiveness: %f",
            optimal_epsilon,
            optimal_aggressiveness,
        )

        logging.info("Applying extreme vector optimization...")
        for page in doc:
            page.clean_contents(sanitize=True)
            shape_aware_extreme_optimization(
                page, optimal_epsilon, optimal_aggressiveness
            )

        logging.info("Aggressively optimizing images...")
        process_images(doc, num_threads=num_threads, grayscale=grayscale)

        temp_pdf_path3: str = create_temp_file(suffix=".pdf").name
        temp_files.append(temp_pdf_path3)

        doc.save(temp_pdf_path3, garbage=4, deflate=True, clean=True)
        doc.close()

        logging.info("Applying final optimizations...")
        gs_params: List[str] = [
            "-sDEVICE=pdfwrite",
            "-dCompatibilityLevel=1.5",
            "-dPDFSETTINGS=/screen",
            "-dColorImageDownsampleType=/Bicubic",
            "-dColorImageResolution=72",
            "-dGrayImageDownsampleType=/Bicubic",
            "-dGrayImageResolution=72",
            "-dMonoImageDownsampleType=/Bicubic",
            "-dMonoImageResolution=72",
            "-dNOPAUSE",
            "-dQUIET",
            "-dBATCH",
            "-dEmbedAllFonts=true",
            "-dSubsetFonts=true",
            "-dCompressFonts=true",
            "-dOptimize=true",
            "-dUseFlateCompression=true",
            "-dAutoFilterColorImages=false",
            "-dAutoFilterGrayImages=false",
            "-dDownsampleMonoImages=false",
            "-dDownsampleGrayImages=false",
            "-dDownsampleColorImages=false",
        ]
        try:
            subprocess.run(
                ["gs"]
                + gs_params
                + [f"-sOutputFile={output_file}", temp_pdf_path3],
                check=True,
            )
        except subprocess.CalledProcessError as e:
            raise PDFProcessingError(
                f"Ghostscript optimization failed: {e.stderr}", ""
            ) from e

        logging.info(
            "Extreme optimization complete. Final file: %s", output_file
        )
    except Exception as e:
        logging.error("Error optimizing PDF: %s", str(e))
        logging.error(traceback.format_exc())
        raise PDFProcessingError("PDF optimization failed", str(e)) from e
    finally:
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)

    original_size = os.path.getsize(input_file)
    optimized_size = os.path.getsize(output_file)
    reduction_percentage = (1 - optimized_size / original_size) * 100
    return original_size, optimized_size, reduction_percentage


def _extreme_optimize_file(args) -> None:
    if args.output is None:
        raise ValueError("Output file must be specified for single file input")
    original_size, optimized_size, reduction_percentage = (
        extreme_pdf_optimization(
            args.input,
            args.output,
            args.threads,
            args.grayscale,
            args.flatten,
        )
    )
    logging.info("Processed: %s", os.path.basename(args.input))
    logging.info("Original size: %d bytes", original_size)
    logging.info("Optimized size: %d bytes", optimized_size)
    logging.info("Size reduction: %.2f%%", reduction_percentage)


def run(args: argparse.Namespace) -> None:
    """
    Run the PDF optimization process based on the provided arguments.

    Args:
        args (argparse.Namespace): The command-line arguments.

    Raises:
        PDFProcessingError: If PDF processing fails.
    """
    try:
        if os.path.isdir(args.input):
            optimize_directory(
                args.input,
                args.output_dir,
                args.threads,
                args.grayscale,
                args.flatten,
            )
        else:
            _extreme_optimize_file(args)
    except Exception as e:  # pylint: disable=broad-except
        raise PDFProcessingError(
            "Error during PDF optimization", traceback.format_exc()
        ) from e


def optimize(args: argparse.Namespace) -> None:
    """
    Optimize the PDF file or directory based on the provided arguments.

    Args:
        args (argparse.Namespace): The command-line arguments.
    """
    try:
        run(args)
    except FileAccessError as e:
        logging.error("File access error: %s", str(e))
        sys.exit(1)
    except PDFProcessingError as e:
        logging.error("PDF processing error: %s", str(e))
        sys.exit(1)
    except Exception as e:  # pylint: disable=broad-except
        logging.error("An unexpected error occurred: %s", str(e))
        logging.error("%s", traceback.format_exc())
        sys.exit(1)


def main() -> None:
    """Run the main optimization process."""
    global cli_args, file_semaphore
    cli_args = parse_arguments()

    # Set up logging
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    # Determine the maximum number of open files allowed by the system
    _soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE)

    # Set the soft limit to the hard limit
    resource.setrlimit(resource.RLIMIT_NOFILE, (hard_limit, hard_limit))

    # Calculate the initial number of concurrent files to process
    current_open_files: int = get_open_files_count()
    available_fds: int = hard_limit - current_open_files
    initial_concurrent_files: int = min(
        int(available_fds * MAX_FD_USAGE_PERCENT / 100), cli_args.max_files
    )

    # Create an adaptive semaphore
    file_semaphore = AdaptiveSemaphore(
        initial_concurrent_files, cli_args.max_files
    )

    logging.info(
        "Initial maximum concurrent files: %d", initial_concurrent_files
    )
    logging.info("Current open files: %d", current_open_files)
    logging.info("Available file descriptors: %d", available_fds)

    try:
        run(cli_args)
    except Exception as e:  # pylint: disable=broad-except
        logging.error("An unexpected error occurred: %s", str(e))
        logging.error("%s", traceback.format_exc())
        sys.exit(1)


def parse_arguments() -> argparse.Namespace:
    """
    Parse command-line arguments for the Advanced PDF Distiller.

    Returns:
        argparse.Namespace: The parsed arguments.
    """
    parser = argparse.ArgumentParser(description="Advanced PDF Distiller")
    parser.add_argument("input", type=str, help="Input PDF file or directory")
    parser.add_argument(
        "output",
        type=str,
        nargs="?",
        help="Output optimized PDF file (for single file input)",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="optimized-pdfs",
        help="Output directory for optimized PDFs (for directory input)",
    )
    parser.add_argument(
        "--threads",
        type=int,
        default=os.cpu_count(),
        help="Number of threads to use",
    )
    parser.add_argument(
        "--grayscale", action="store_true", help="Convert images to grayscale"
    )
    parser.add_argument(
        "--flatten",
        action="store_true",
        help="Flatten form fields and annotations",
    )
    parser.add_argument(
        "--max_files",
        type=int,
        default=100,
        help="Maximum number of files to process concurrently",
    )
    return parser.parse_args()


if __name__ == "__main__":
    main()
