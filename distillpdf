#!/usr/bin/env python3
"""
DistillPDF.

This script provides comprehensive functionalities to optimize and transform
PDF files. It includes capabilities for resizing and compressing images within
PDFs, converting images to grayscale, flattening form fields and annotations,
and aggressively subsetting embedded fonts to reduce file size.
Vector graphics within PDFs are optimized for minimal visual alteration using
Bayesian optimization techniques. Additional functionalities include removing
metadata for privacy, cleaning unused resources, and optimizing the entire
content stream of PDF pages.

Features:
    - Image resizing and compression to predefined dimensions and quality.
    - Conversion of colored images to grayscale to reduce complexity.
    - Flattening of PDF annotations and form fields to static images.
    - Font subsetting to include only the characters used in the document.
    - Metadata removal for enhanced privacy and security.
    - Cleanup of unreferenced PDF resources to shrink file size.
    - Content stream optimization to strip unnecessary data.
    - Vector graphic simplification based on adjustable precision levels.
    - Multi-threaded processing for performance optimization.
    - Detailed logging of processes and errors for troubleshooting and analysis.

The script utilizes a combination of libraries including PyMuPDF (fitz), pikepdf,
PIL, OpenCV, scikit-learn, scipy, and numpy to perform its operations, integrating
advanced image processing, machine learning for optimization decisions, and direct
PDF manipulation capabilities.

Setup:
    Ensure you have Python version 3.8.x installed.
    
    $ pip install -r requirements.txt
    $ chmod +x distillpdf
    
    (Recommended) Move the distillpdf script to a directory in your PATH, and modify
    this file's shebang to point to your compatible Python binary location.

Usage:
    To use the script, provide an input PDF file and an output destination, along with
    optional flags for grayscale conversion, flattening, and specifying the number of
    processing threads. The script is designed to be run as a script and can be invoked
    from the command line.

Example:
    $ ./distillpdf input.pdf output.pdf --threads=4 --grayscale --flatten

Design:
    This script is designed for high-performance environments where precision and file
    size reduction are critical, suitable for both commercial and academic applications.

Author:
    Juan Sugg Gilbert
    juanpedrosugg [at] gmail [dot] com

License:
    This script is licensed under the GNU Affero General Public License v3.0
"""
from multiprocessing.pool import ApplyResult
import os
import sys
import traceback
import io
import subprocess
import argparse
import tempfile
import logging
import shutil
from typing import Any, List, Tuple, Set, Optional, Union, Callable
import math
import random
import concurrent.futures
from functools import lru_cache
import multiprocessing
import difflib

import numpy as np
import cv2
from cv2.typing import MatLike
from scipy.stats import norm
from scipy.optimize import minimize, OptimizeResult
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, Sum, WhiteKernel
from skimage.metrics import structural_similarity as ssim
import pytesseract
from PIL import Image
from fontTools import subset
from fontTools.ttLib import TTFont
import pikepdf
import fitz  # PyMuPDF

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class PDFProcessingError(Exception):
    """Base class for exceptions in the Distill PDF module."""

    def __init__(self, message: str, context: str) -> None:
        """
        Initialize the PDFProcessingError.

        Args:
            message (str): The error message.
            context (str): Additional context about the error.
        """
        super().__init__(f"{message} - Context: {context}")
        self.context: str = context

    def get_message_chain(self) -> List[str]:
        """
        Return a list of messages representing the exception chain.

        Returns:
            List[str]: A list of error messages.
        """
        messages: List[str] = [str(self)]
        cause: Optional[BaseException] = self.__cause__
        while cause:
            if isinstance(cause, PDFProcessingError):
                messages.extend(cause.get_message_chain())
            else:
                messages.append(str(cause))
            cause = getattr(cause, "__cause__", None)
        return messages


class FileAccessError(PDFProcessingError):
    """Exception raised for errors accessing input or output files."""


class TextDetectionError(PDFProcessingError):
    """Exception raised for errors during text detection in images."""


class ImageProcessingError(PDFProcessingError):
    """Exception raised for errors during image processing."""


class RasterOptimizationError(PDFProcessingError):
    """Exception raised for errors during raster image optimization."""


class VectorOptimizationError(PDFProcessingError):
    """Exception raised for errors during vector optimization."""


class FontSubsettingError(PDFProcessingError):
    """Exception raised for errors during font subsetting."""


class ContentOptimizationError(PDFProcessingError):
    """Exception raised for errors during content stream optimization."""


def safe_file_operation(func: Any) -> Any:
    """Decorator to safely handle file operations."""

    def wrapper(*args: Any, **kwargs: Any) -> Any:
        try:
            return func(*args, **kwargs)
        except OSError as e:
            raise FileAccessError(
                f"File operation failed: {func.__name__}", str(e)
            ) from e

    return wrapper


@safe_file_operation
def create_temp_file(suffix: str) -> tempfile._TemporaryFileWrapper:
    """
    Safely create a named temporary file.

    Args:
        suffix (str): The file suffix.

    Returns:
        tempfile._TemporaryFileWrapper: A temporary file object.
    """
    return tempfile.NamedTemporaryFile(suffix=suffix, delete=False)


def get_text_from_image(image: np.ndarray) -> str:
    """
    Extract text from an image using OCR.

    Args:
        image (np.ndarray): The input image.

    Returns:
        str: The extracted text.

    Raises:
        ImageProcessingError: If text extraction fails.
    """
    try:
        return pytesseract.image_to_string(
            Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        )
    except Exception as e:
        raise ImageProcessingError(
            "Failed to extract text from image", str(e)
        ) from e


def is_ocr_readable(
    original_text: str,
    compressed_image: np.ndarray,
    similarity_threshold: float = 0.8,
) -> bool:
    """
    Check if the OCR text extracted from a compressed image is readable
    compared to the original text.

    Args:
        original_text (str): The original text.
        compressed_image (np.ndarray): The compressed image.
        similarity_threshold (float, optional): The similarity threshold.
                                                Defaults to 0.8.

    Returns:
        bool: True if the compressed image is OCR readable, False otherwise.

    Raises:
        ImageProcessingError: If OCR readability check fails.
    """
    try:
        compressed_text: str = get_text_from_image(compressed_image)
        similarity: float = difflib.SequenceMatcher(
            None, original_text, compressed_text
        ).ratio()
        return similarity >= similarity_threshold
    except ImageProcessingError as e:
        raise ImageProcessingError(
            "Failed to check OCR readability", str(e)
        ) from e


def optimize_raster_image(
    image_bytes: bytes, max_width: int = 1000, quality: int = 75
) -> bytes:
    """
    Optimize a raster image by resizing and compressing it using JPEG.

    Args:
        image_bytes (bytes): The input image bytes.
        max_width (int, optional): The maximum width of the optimized image.
                                   Defaults to 1000.
        quality (int, optional): The JPEG quality (0-100). Defaults to 75.

    Returns:
        bytes: The optimized image bytes.

    Raises:
        ImageProcessingError: If image optimization fails.
    """
    try:
        nparr: np.ndarray = np.frombuffer(image_bytes, np.uint8)
        img: MatLike = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)
        h, w = img.shape[:2]
        if w > max_width:
            ratio: float = max_width / w
            new_h = int(h * ratio)
            img: MatLike = cv2.resize(
                img, (max_width, new_h), interpolation=cv2.INTER_AREA
            )

        _, optimized_img = cv2.imencode(
            ".jpg", img, [cv2.IMWRITE_JPEG_QUALITY, quality]
        )
        if optimized_img is None:
            raise ImageProcessingError(
                "Failed to encode optimized image", "cv2.imencode"
            )
        return optimized_img.tobytes()
    except Exception as e:
        raise ImageProcessingError(
            "Error optimizing raster image", str(e)
        ) from e


def get_image_in_bytes_representation(image: fitz.Pixmap) -> bytes:
    """
    Get the byte representation of an image in PNG format.

    Args:
        image (fitz.Pixmap): The input image.

    Returns:
        bytes: The image bytes in PNG format.

    Raises:
        ImageProcessingError: If conversion to bytes fails.
    """
    try:
        return image.tobytes("png")
    except Exception as e:
        raise ImageProcessingError(
            "Failed to convert image to bytes", str(e)
        ) from e


def is_rect_finite(rect: fitz.Rect) -> bool:
    """
    Check if all coordinates of a rectangle are finite.

    Args:
        rect (fitz.Rect): The rectangle to check.

    Returns:
        bool: True if all coordinates are finite, False otherwise.
    """
    return all(
        math.isfinite(coord) for coord in [rect.x0, rect.y0, rect.x1, rect.y1]
    )


def get_dpi(image_path: str, default_dpi: int = 300) -> int:
    """
    Attempt to determine the DPI from the image metadata or
    return a default.

    Args:
        image_path (str): The path to the image file.
        default_dpi (int, optional): The default DPI to useif metadata
                                     is not available. Defaults to 300.

    Returns:
        int: The determined DPI.

    Raises:
        ImageProcessingError: If DPI determination fails.
    """
    try:
        with Image.open(image_path) as img:
            dpi: Any = img.info.get("dpi", (default_dpi, default_dpi))
            logging.debug("get_dpi -> DPI: %s", dpi)
            return (
                int((dpi[0] + dpi[1]) // 2)
                if isinstance(dpi, tuple)
                else default_dpi
            )
    except Exception as e:
        logging.error("Error determining DPI: %s", e)
        raise ImageProcessingError(
            "Failed to determine DPI.", f"Error in get_dpi function: {str(e)}"
        ) from e


def get_image_rect(page: fitz.Page, img_xref: int) -> Optional[fitz.Rect]:
    """
    Get the rectangle of an image on a PDF page.

    Args:
        page (fitz.Page): The PDF page.
        img_xref (int): The image reference.

    Returns:
        Optional[fitz.Rect]: The image rectangle, or None if not found.

    Raises:
        ImageProcessingError: If image rectangle calculation fails.
    """
    try:
        img_list: List[Tuple] = page.get_images(full=True)
        for img in img_list:
            if img[0] == img_xref:
                img_rect: fitz.Rect = page.get_image_bbox(img)  # type: ignore # Given the current arguments, this is safe
                if img_rect.is_empty or img_rect.is_infinite:
                    logging.debug(
                        "get_image_rect -> Image rectangle is empty or not finite, skipping."
                    )
                    return None
                return img_rect
        return None
    except Exception as e:
        logging.error(
            "Failed to calculate image rectangle for xref %d: %s",
            img_xref,
            str(e),
        )
        raise ImageProcessingError(
            f"Failed to get image rectangle for xref {img_xref}", str(e)
        ) from e


def _insert_optimized_image(img_xref, img_rect, image_bytes, page) -> None:
    """
    Inserts an optimized image into a PDF page.

    Args:
        img_xref (int): The xref of the image.
        img_rect (Rectangle): The rectangle where the image will be inserted.
        image_bytes (bytes): The bytes of the image to be inserted.
        page: The PDF page to insert the image into.

    Returns:
        None

    Raises:
        ValueError: If the rectangle is not finite or empty.
    """
    logging.debug("Image rect for xref %s: %s", img_xref, img_rect)

    if not is_rect_finite(img_rect):
        raise ValueError("Rectangle is not finite.")

    if img_rect.is_empty:
        raise ValueError("Rectangle is empty.")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp_file:
        temp_file.write(image_bytes)
        temp_file.flush()
        temp_file_path = temp_file.name

    page.insert_image(img_rect, filename=temp_file.name, keep_proportion=True)  # type: ignore # pymupdf library does not expose this method

    os.unlink(temp_file_path)


def replace_image_with_optimized_version(
    page: fitz.Page, image_bytes: bytes, img_rect: fitz.Rect, img_xref: int
) -> None:
    """
    Replace the image with an optimized version using JPEG.

    Args:
        page (fitz.Page): The PDF page.
        image_bytes (bytes): The optimized image bytes.
        img_rect (fitz.Rect): The image rectangle.
        img_xref (int): The image reference.

    Raises:
        ValueError: If the image rectangle is invalid.
    """
    try:
        _insert_optimized_image(img_xref, img_rect, image_bytes, page)
    except Exception as e:
        logging.debug(
            "Failed to insert or delete image with xref = %d: %s",
            img_xref,
            str(e),
        )
        raise PDFProcessingError(
            f"Failed to insert or delete image with xref {img_xref}", str(e)
        ) from e

    try:
        page.delete_image(img_xref)  # type: ignore  # pymupdf library does not expose this method
    except Exception as e:
        logging.debug(
            "Failed to delete the original image with xref = %d: %s",
            img_xref,
            str(e),
        )
        raise PDFProcessingError(
            f"Failed to delete the original image with xref {img_xref}", str(e)
        ) from e


def replace_image(
    doc: fitz.Document,
    img_xref: int,
    page_number: int,
    temp_image_path: str,
    grayscale: bool,
) -> None:
    """
    Replace a raster image on a PDF page with an optimized image.

    Args:
        doc (fitz.Document): The PDF document.
        img_xref (int): The image reference.
        page_number (int): The page number.
        temp_image_path (str): The path to the temporary image file.
        grayscale (bool): Whether to convert the image to grayscale.

    Raises:
        PDFProcessingError: If image replacement fails.
    """
    try:
        page: fitz.Page = doc[page_number]
        base_image: fitz.Pixmap = fitz.Pixmap(doc, img_xref)

        if grayscale and base_image.n > 2:
            base_image = fitz.Pixmap(fitz.csGRAY, base_image)

        img_rect: Optional[fitz.Rect] = get_image_rect(page, img_xref)
        if img_rect is None:
            raise PDFProcessingError(
                f"Failed to get image rectangle for image {img_xref}",
                "Image not found on page",
            )

        image_bytes: bytes = base_image.tobytes("png")

        try:
            logging.debug(
                "Original image size for image %d: %d bytes",
                {img_xref},
                {len(image_bytes)}
            )
            optimized_image_bytes: bytes = optimize_raster_image(image_bytes)
            logging.debug(
                "Optimized image size for image %d: %d bytes",
                    {img_xref},
                    {len(optimized_image_bytes)}
            )

            if len(optimized_image_bytes) >= len(image_bytes):
                logging.info(
                    "Original image with xref = %d was already highly optimized."
                    "Keeping original.",
                    img_xref,
                )
                return

            replace_image_with_optimized_version(
                page, optimized_image_bytes, img_rect, img_xref
            )
        except Exception as e:
            raise ImageProcessingError(
                "Failed during fallback image replacement", str(e)
            ) from e

        page.delete_image(img_xref)  # type: ignore  # pymupdf library does not expose this method
        logging.info(
            "Successfully replaced image %d on page %d with an optimized version.",
            img_xref,
            page_number,
        )
    except Exception as e:
        raise PDFProcessingError(
            f"Unexpected error while replacing image {img_xref} on page {page_number}",
            f"Error: {str(e)}\nTraceback: {traceback.format_exc()}",
        ) from e
    finally:
        if temp_image_path and os.path.exists(temp_image_path):
            try:
                os.remove(temp_image_path)
            except Exception as e:  # pylint: disable=broad-except
                logging.warning(
                    "Failed to remove temporary file %s: %s",
                    temp_image_path,
                    str(e),
                )


def _log_process_image_pdf_processing_error(
    page_num: int, e: PDFProcessingError, xref: int
) -> None:
    """
    Log a PDFProcessingError that occurred while processing an image.

    Args:
        page_num (int): The page number where the error occurred.
        e (PDFProcessingError): The error that occurred.
        xref (int): The image reference.
    """
    logging.debug(
        "PDFProcessingError while processing image from page %d: %s",
        page_num,
        str(e),
    )
    logging.debug("Error: %s", str(e))
    logging.debug("Image xref: %i", xref)
    logging.debug("Cause: %s", str(e.__cause__))
    logging.debug("Context: %s", e.context)
    logging.info("Keeping the original image %d.", xref)


def process_image(args: Tuple[fitz.Document, int, int, str, bool]) -> None:
    """
    Process a single image in a PDF document.

    Args:
        args (Tuple[fitz.Document, int, int, str, bool]): A tuple containing:
            - The PDF document
            - The image reference
            - The page number
            - The path to the temporary image file
            - A boolean indicating whether to convert to grayscale
    """
    doc, xref, page_num, image_path, grayscale = args
    try:
        replace_image(doc, xref, page_num, image_path, grayscale)
    except PDFProcessingError as e:
        _log_process_image_pdf_processing_error(page_num, e, xref)
    except Exception as e:  # pylint: disable=broad-except
        logging.error(
            "Unexpected error while processing image from page %d: %s",
            page_num,
            str(e),
        )
        logging.debug("Traceback: %s", traceback.format_exc())
        logging.info("Keeping original image %d", xref)


def process_images(
    doc: fitz.Document, num_threads: int, grayscale: bool
) -> None:
    """
    Process all images in a PDF document using multiple threads.

    Args:
        doc (fitz.Document): The PDF document.
        num_threads (int): The number of threads to use for processing.
        grayscale (bool): Whether to convert images to grayscale.

    Raises:
        PDFProcessingError: If an error occurs during image processing.
    """
    image_tasks: List[Tuple[fitz.Document, int, int, str, bool]] = []

    try:
        # pylint: disable=consider-using-enumerate  # fitz.Document is not enumerable
        for page_num in range(len(doc)):
            page: fitz.Page = doc[page_num]
            for img in page.get_images(full=True):
                img_xref = img[0]
                pixmap = fitz.Pixmap(doc, img_xref)
                temp_file = create_temp_file(suffix=".png")
                pixmap.save(temp_file.name)
                image_tasks.append(
                    (
                        doc,
                        int(img_xref),
                        page_num,
                        str(temp_file.name),
                        grayscale,
                    )
                )
        # pylint: enable=consider-using-enumerate
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=num_threads
        ) as executor:
            list(executor.map(process_image, image_tasks))
    except Exception as e:
        raise PDFProcessingError(
            "Error processing images", traceback.format_exc()
        ) from e


def get_text_from_page(page: fitz.Page) -> str:
    """
    Extract text content from a PDF page.

    Args:
        page (fitz.Page): The PDF page.

    Returns:
        str: The extracted text.

    Raises:
        PDFProcessingError: If text extraction fails.
    """
    try:
        return page.get_text()  # type: ignore  # pymupdf library does not expose this method
    except Exception as e:
        raise PDFProcessingError(
            f"Failed to extract text from page: {str(e)}", ""
        ) from e


def simplify_vector_path(
    points: List[Tuple[float, float]], epsilon: float
) -> List[Tuple[float, float]]:
    """
    Simplify a vector path by reducing the number of points based
    on the given epsilon value.

    Args:
        points (List[Tuple[float, float]]): The original path points.
        epsilon (float): The simplification threshold.

    Returns:
        List[Tuple[float, float]]: The simplified path points.
    """

    def point_line_distance(
        point: Tuple[float, float],
        start: Tuple[float, float],
        end: Tuple[float, float],
    ) -> float:
        if start == end:
            return math.hypot(point[0] - start[0], point[1] - start[1])
        n: float = abs(
            (end[0] - start[0]) * (start[1] - point[1])
            - (start[0] - point[0]) * (end[1] - start[1])
        )
        d: float = math.hypot(end[0] - start[0], end[1] - start[1])
        return n / d

    if len(points) < 3:
        return points

    dmax: float = 0.0
    index: int = 0
    for i in range(1, len(points) - 1):
        d: float = point_line_distance(points[i], points[0], points[-1])
        if d > dmax:
            index = i
            dmax = d

    if dmax > epsilon:
        results1: List[Tuple[float, float]] = simplify_vector_path(
            points[: index + 1], epsilon
        )
        results2: List[Tuple[float, float]] = simplify_vector_path(
            points[index:], epsilon
        )
        results: List[Tuple[float, float]] = results1[:-1] + results2
    else:
        results = [points[0], points[-1]]

    return results


def extreme_vector_optimization(
    page: fitz.Page, epsilon: float
) -> Tuple[int, int]:
    """
    Perform extreme vector path optimization on a PDF page.

    Args:
        page (fitz.Page): The PDF page to optimize.
        epsilon (float): The simplification threshold.

    Returns:
        Tuple[int, int]: The total number of points before and
                         after optimization.

    Raises:
        VectorOptimizationError: If an error occurs during
                                 vector optimization.
    """
    try:
        shape: Any = page.new_shape()  # type: ignore  # pymupdf library does not expose this method
        total_points_before: int = 0
        total_points_after: int = 0
        for path in page.get_drawings():
            if path["type"] in ["l", "c"]:  # line or curve
                points: List[Tuple[float, float]] = path["pts"]
                total_points_before += len(points)
                simplified_points: List[Tuple[float, float]] = (
                    simplify_vector_path(points, epsilon)
                )
                total_points_after += len(simplified_points)
                if path["type"] == "l" and len(simplified_points) >= 2:
                    shape.draw_polyline(simplified_points)
                elif path["type"] == "c" and len(simplified_points) >= 4:
                    shape.draw_bezier(simplified_points)
            elif path["type"] in ["f", "s"]:  # fill or stroke
                shape.finish(fill=path["type"] == "f", color=path.get("color"))
            elif path["type"] == "e":  # ellipse
                rect: fitz.Rect = fitz.Rect(path["rect"])
                shape.draw_oval(rect)

        shape.commit()
        return total_points_before, total_points_after
    except Exception as e:
        raise VectorOptimizationError(
            f"Error during extreme vector optimization: {str(e)}",
            traceback.format_exc(),
        ) from e


def image_similarity(img1: Image.Image, img2: Image.Image) -> float:
    """
    Calculate the structural similarity index between two images.

    Args:
        img1 (Image.Image): The first image.
        img2 (Image.Image): The second image.

    Returns:
        float: The structural similarity index.

    Raises:
        ImageProcessingError: If similarity calculation fails.
    """
    try:
        img1_gray: Image.Image = img1.convert("L")
        img2_gray: Image.Image = img2.convert("L")

        if img1_gray.size != img2_gray.size:
            img2_gray = img2_gray.resize(img1_gray.size)

        img1_array: np.ndarray = np.array(img1_gray)
        img2_array: np.ndarray = np.array(img2_gray)

        similarity: float
        _: Any
        similarity, _ = ssim(img1_array, img2_array, full=True)
        return similarity
    except Exception as e:
        raise ImageProcessingError(
            f"Failed to calculate image similarity: {str(e)}", ""
        ) from e


def calculate_ei(
    mu: np.ndarray, mu_sample_opt: float, xi: float, sigma: np.ndarray
) -> np.ndarray:
    """
    Calculate the expected improvement values for Bayesian optimization.

    Args:
        mu (np.ndarray): The mean predictions.
        mu_sample_opt (float): The current best observed value.
        xi (float): Exploration-exploitation trade-off parameter.
        sigma (np.ndarray): The standard deviations of the predictions.

    Returns:
        np.ndarray: The expected improvement values.
    """
    imp: np.ndarray = mu - mu_sample_opt - xi
    z: np.ndarray = np.divide(
        imp, sigma, out=np.zeros_like(sigma), where=sigma != 0
    )
    result: np.ndarray = imp * norm.cdf(z) + sigma * norm.pdf(z)
    result[sigma == 0.0] = 0.0
    return result


def expected_improvement(
    x: np.ndarray,
    x_sample: np.ndarray,
    _y_sample: np.ndarray,
    gpr: GaussianProcessRegressor,
    xi: float = 0.01,
) -> np.ndarray:
    """
    Calculate the expected improvement for Bayesian optimization.

    Args:
        x (np.ndarray): The input points.
        x_sample (np.ndarray): The sample points.
        _y_sample (np.ndarray): The sample values (unused).
        gpr (GaussianProcessRegressor): The Gaussian Process Regressor.
        xi (float, optional): Exploration-exploitation trade-off parameter.
                              Defaults to 0.01.

    Returns:
        np.ndarray: The expected improvement values.
    """
    mu: Union[Any, np.ndarray]
    sigma: Union[Any, np.ndarray]
    mu, sigma = gpr.predict(x.reshape(-1, 1), return_std=True)  # type: ignore  # given the current arguments, this is safe
    mu_sample: Union[np.ndarray, Tuple[np.ndarray]] = gpr.predict(x_sample)  # type: ignore  # given the current arguments, this is safe

    sigma = sigma.reshape(-1, 1)
    mu_sample_opt: float = np.max(mu_sample)

    with np.errstate(divide="warn"):
        ei: np.ndarray = calculate_ei(mu, mu_sample_opt, xi, sigma)
    return ei.ravel()


def propose_location(
    acquisition: Callable[..., np.ndarray],
    x_sample: np.ndarray,
    y_sample: np.ndarray,
    gpr: GaussianProcessRegressor,
    bounds: np.ndarray,
    n_restarts: int = 25,
) -> Optional[np.ndarray]:
    """
    Propose the next sample location for Bayesian optimization.

    Args:
        acquisition (Callable[..., np.ndarray]): The acquisition function.
        x_sample (np.ndarray): The sample points.
        y_sample (np.ndarray): The sample values.
        gpr (GaussianProcessRegressor): The Gaussian Process Regressor.
        bounds (np.ndarray): The bounds for optimization.
        n_restarts (int, optional): The number of optimization restarts.
                                    Defaults to 25.

    Returns:
        Optional[np.ndarray]: The proposed location, or None
                              if optimization fails.
    """
    min_val: float = float("inf")
    min_x: Optional[np.ndarray] = None

    def min_obj(x: np.ndarray) -> np.ndarray:
        return -acquisition(x.reshape(1, -1), x_sample, y_sample, gpr).ravel()

    for x0 in np.random.uniform(
        bounds[:, 0], bounds[:, 1], size=(n_restarts, 1)
    ):
        res: OptimizeResult = minimize(
            min_obj, x0=x0, bounds=bounds, method="L-BFGS-B"
        )
        if res.fun < min_val:
            min_val = (
                res.fun[0] if isinstance(res.fun, np.ndarray) else res.fun
            )
            min_x = res.x

    return min_x


def bayesian_optimization(
    objective: Callable[[float], float],
    bounds: np.ndarray,
    n_iterations: int,
    n_initial: int = 5,
) -> Tuple[float, float]:
    """
    Perform Bayesian optimization to find the optimal value for a given
    objective function.

    Args:
        objective (Callable[[float], float]): The objective function to optimize.
        bounds (np.ndarray): The bounds for optimization.
        n_iterations (int): The number of optimization iterations.
        n_initial (int, optional): The number of initial random samples.
                                   Defaults to 5.

    Returns:
        Tuple[float, float]: The best input value and its corresponding
                             objective value.
    """
    dim: int = bounds.shape[0]
    x_sample: np.ndarray = np.random.uniform(
        bounds[:, 0], bounds[:, 1], size=(n_initial, dim)
    )
    y_sample: np.ndarray = np.array([objective(x[0]) for x in x_sample])

    kernel: Sum = Matern(
        nu=2.5, length_scale=1.0, length_scale_bounds=(1e-2, 1e5)
    ) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-12, 1e2))
    gpr: GaussianProcessRegressor = GaussianProcessRegressor(
        kernel=kernel, n_restarts_optimizer=25, random_state=42
    )

    for i in range(n_iterations):
        gpr.fit(x_sample, y_sample)

        next_sample: Optional[np.ndarray] = propose_location(
            expected_improvement, x_sample, y_sample, gpr, bounds
        )

        if next_sample is None:
            logging.warning(
                "Optimization stopped at iteration %d due to "
                "failure in proposing next sample.",
                (i + 1),
            )
            break

        next_sample = next_sample.ravel()
        next_objective: float = objective(next_sample[0])
        logging.info(
            "Iteration %d/%d: epsilon = %.4f, similarity = %.4f",
            i + 1,
            n_iterations,
            next_sample[0],
            next_objective,
        )

        x_sample = np.vstack((x_sample, next_sample))
        y_sample = np.append(y_sample, next_objective)

    best_idx: int = int(np.argmax(y_sample))
    return float(x_sample[best_idx, 0]), float(y_sample[best_idx])


def find_optimal_vector_simplification(
    doc: fitz.Document,
    min_epsilon: float = 0.1,
    max_epsilon: float = 10.0,
    n_iterations: int = 20,
    sample_size: int = 5,
    similarity_threshold: float = 0.8,
) -> float:
    """
    Find the optimal vector simplification epsilon value for a
    PDF document using Bayesian optimization.

    Args:
        doc (fitz.Document): The PDF document.
        min_epsilon (float, optional): The minimum epsilon value.
                                       Defaults to 0.1.
        max_epsilon (float, optional): The maximum epsilon value.
                                       Defaults to 10.0.
        n_iterations (int, optional): The number of optimization iterations.
                                      Defaults to 20.
        sample_size (int, optional): The number of pages to sample.
                                     Defaults to 5.
        similarity_threshold (float, optional): The similarity threshold.
                                                Defaults to 0.8.

    Returns:
        float: The optimal epsilon value for vector simplification.

    Raises:
        VectorOptimizationError: If an error occurs during optimization.
    """
    try:
        num_pages: int = len(doc)
        sample_pages: List[int] = sorted(
            random.sample(range(num_pages), min(sample_size, num_pages))
        )

        initial_similarity: float
        _: Any
        _, initial_similarity = optimize_and_compare(
            doc, min_epsilon, sample_pages
        )
        if initial_similarity == 1.0:
            logging.info(
                "No vector content to optimize. Skipping vector simplification."
            )
            return min_epsilon
    except Exception as e:
        raise VectorOptimizationError(
            "Error in find_optimal_vector_simplification",
            traceback.format_exc(),
        ) from e

    def objective(epsilon: float) -> float:
        """
        Calculate the objective function value based on the similarity ratio
        obtained from optimizing and comparing a PDF document.

        Args:
            epsilon (float): The epsilon value for vector simplification.

        Returns:
            float: The similarity ratio.
        """
        try:
            similarity: float
            _: Any
            _, similarity = optimize_and_compare(doc, epsilon, sample_pages)
            logging.info(
                "Objective evaluation: epsilon = %.4f, similarity = %.4f",
                epsilon,
                similarity,
            )
            return similarity
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Error in objective function: %s", str(e))
            return 0.0

    logging.info("Starting Bayesian optimization for vector simplification")

    try:
        bounds: np.ndarray = np.array([[min_epsilon, max_epsilon]])
        best_epsilon: float
        best_similarity: float
        best_epsilon, best_similarity = bayesian_optimization(
            objective, bounds, n_iterations
        )

        logging.info(
            "Optimization complete. Best epsilon: %.4f, Best similarity: %.4f",
            best_epsilon,
            best_similarity,
        )

        if best_similarity < similarity_threshold:
            logging.warning(
                "Best similarity (%.4f) is below the threshold (%.4f)."
                "Results may not be optimal.",
                best_similarity,
                similarity_threshold,
            )

        return float(best_epsilon)
    except Exception as e:
        raise VectorOptimizationError(
            "Error in find_optimal_vector_simplification",
            traceback.format_exc(),
        ) from e


def optimize_and_compare(
    doc: fitz.Document, epsilon: float, sample_pages: List[int]
) -> Tuple[float, float]:
    """
    Optimize vector paths on selected pages of a PDF document and compare
    the optimization results.

    Args:
        doc (fitz.Document): The PDF document.
        epsilon (float): The epsilon value for vector simplification.
        sample_pages (List[int]): The list of page numbers to sample.

    Returns:
        Tuple[float, float]: The epsilon value and the similarity ratio.

    Raises:
        VectorOptimizationError: If an error occurs during optimization
                                 or comparison.
    """
    try:
        temp_doc: fitz.Document = fitz.open()
        temp_doc.insert_pdf(
            doc, from_page=min(sample_pages), to_page=max(sample_pages)
        )

        total_points_before: int = 0
        total_points_after: int = 0

        for page in temp_doc:
            points_before: int
            points_after: int
            points_before, points_after = extreme_vector_optimization(
                page, epsilon
            )
            total_points_before += points_before
            total_points_after += points_after

        similarity: float = (
            1.0
            if total_points_before == 0
            else total_points_after / total_points_before
        )

        return epsilon, similarity
    except Exception as e:
        raise VectorOptimizationError(
            "Error in optimize_and_compare", traceback.format_exc()
        ) from e


def get_used_characters(doc: fitz.Document) -> Set[str]:
    """
    Get the set of used characters from a PDF document.

    Args:
        doc (fitz.Document): The PDF document.

    Returns:
        Set[str]: The set of used characters.

    Raises:
        PDFProcessingError: If character extraction fails.
    """
    try:
        used_chars: Set[str] = set()
        for page in doc:
            text: str = page.get_text()  # type: ignore  # pymupdf library does not expose this method
            used_chars.update(set(text))
        return used_chars
    except Exception as e:
        raise PDFProcessingError(
            "Failed to extract used characters", str(e)
        ) from e


def aggressive_font_subset(pdf: pikepdf.Pdf, used_chars: Set[str]) -> None:
    """
    Perform aggressive font subsetting on a PDF object based on the
    used characters.

    Args:
        pdf (pikepdf.Pdf): The PDF object.
        used_chars (Set[str]): The set of used characters.

    Raises:
        FontSubsettingError: If font subsetting fails.
    """
    for page in pdf.pages:
        if "/Resources" in page and "/Font" in page["/Resources"]:
            fonts: pikepdf.Object = page["/Resources"]["/Font"]
            for _font_key, font in fonts.items():
                if "/FontDescriptor" in font:
                    descriptor: pikepdf.Object = font["/FontDescriptor"]
                    for key in ["/FontFile", "/FontFile2", "/FontFile3"]:
                        if key in descriptor:
                            font_file: pikepdf.Object = descriptor[key]
                            if isinstance(font_file, pikepdf.Stream):
                                font_data: bytes = font_file.read_bytes()
                                try:
                                    ttfont: TTFont = TTFont(
                                        io.BytesIO(font_data)
                                    )
                                    subsetter: subset.Subsetter = (
                                        subset.Subsetter()
                                    )
                                    subsetter.populate(
                                        text="".join(used_chars)
                                    )
                                    subsetter.subset(ttfont)
                                    new_font_buffer: io.BytesIO = io.BytesIO()
                                    ttfont.save(new_font_buffer)
                                    new_font_data: bytes = (
                                        new_font_buffer.getvalue()
                                    )
                                    descriptor[key] = pikepdf.Stream(
                                        pdf, new_font_data
                                    )
                                except Exception as e:
                                    raise FontSubsettingError(
                                        f"Failed to subset font: {str(e)}",
                                        traceback.format_exc(),
                                    ) from e


def remove_metadata(pdf: pikepdf.Pdf) -> None:
    """
    Remove metadata from a PDF object.

    Args:
        pdf (pikepdf.Pdf): The PDF object.
    """
    with pdf.open_metadata() as meta:
        meta.clear()
    if pdf.Root.get("/Metadata") is not None:
        del pdf.Root["/Metadata"]


def clean_pdf(pdf: pikepdf.Pdf) -> None:
    """
    Clean up a PDF object by removing unreferenced resources
    and indirect objects from pages.

    Args:
        pdf (pikepdf.Pdf): The PDF object.

    Raises:
        PDFProcessingError: If PDF cleaning fails.
    """
    try:
        pdf.remove_unreferenced_resources()
        for page in pdf.pages:
            if "/Resources" in page:
                resources: pikepdf.Object = page["/Resources"]
                for resource_type in [
                    "/XObject",
                    "/ExtGState",
                    "/Pattern",
                    "/Shading",
                ]:
                    if resource_type in resources:
                        resource_dict: pikepdf.Object = resources[
                            resource_type
                        ]
                        keys_to_delete: List[str] = []
                        for key, value in resource_dict.items():
                            if value.is_indirect:
                                try:
                                    _: pikepdf.Object = pdf.get_object(
                                        value.objgen
                                    )
                                except KeyError:
                                    keys_to_delete.append(key)
                        for key in keys_to_delete:
                            del resource_dict[key]
    except Exception as e:
        raise PDFProcessingError(
            "Failed to clean PDF", traceback.format_exc()
        ) from e


def flatten_annotations(doc: fitz.Document) -> None:
    """
    Flatten annotations in a PDF document.

    Args:
        doc (fitz.Document): The PDF document.

    Raises:
        PDFProcessingError: If annotation flattening fails.
    """
    try:
        for page in doc:
            for annot in page.annots():
                if annot.type[0] in [
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    12,
                    13,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19,
                    20,
                ]:  # Form fields and annotations
                    page.add_redact_annot(annot.rect)
                    page.apply_redactions()  # type: ignore  # pymupdf library does not expose this method
    except Exception as e:
        raise PDFProcessingError(
            "Failed to flatten annotations", traceback.format_exc()
        ) from e


@lru_cache(maxsize=1000)
def optimize_content_stream(content: bytes) -> bytes:
    """
    Optimize a content stream by removing empty lines and
    lines starting with '%'.

    Args:
        content (bytes): The content stream.

    Returns:
        bytes: The optimized content stream.
    """
    return b"\n".join(
        line
        for line in content.split(b"\n")
        if line.strip() and not line.strip().startswith(b"%")
    )


def process_content(content: bytes) -> bytes:
    """
    Optimize the content streams of a PDF.

    Args:
        content (bytes): The content stream.

    Returns:
        bytes: The optimized content stream.

    Raises:
        ContentOptimizationError: If content optimization fails.
    """
    try:
        return optimize_content_stream(content)
    except Exception as e:
        raise ContentOptimizationError(
            f"Failed to optimize content stream: {str(e)}",
            traceback.format_exc(),
        ) from e


def optimize_pdf_contents(pdf: pikepdf.Pdf) -> None:
    """
    Optimize the content streams of a PDF file.

    Args:
        pdf (pikepdf.Pdf): The PDF object.

    Raises:
        ContentOptimizationError: If content optimization fails.
    """
    with multiprocessing.Pool() as pool:
        for page in pdf.pages:
            if "/Contents" in page:
                contents: pikepdf.Object = page["/Contents"]
                try:
                    if isinstance(contents, pikepdf.Stream):
                        optimized: ApplyResult = pool.apply_async(
                            process_content, (contents.read_bytes(),)
                        )
                        page["/Contents"] = pikepdf.Stream(
                            pdf, optimized.get()
                        )
                    elif isinstance(contents, pikepdf.Array):
                        optimized_contents: List[pikepdf.Stream] = []
                        for content in contents:  # type: ignore
                            if isinstance(content, pikepdf.Stream):
                                optimized = pool.apply_async(
                                    process_content, (content.read_bytes(),)
                                )
                                optimized_contents.append(
                                    pikepdf.Stream(pdf, optimized.get())
                                )
                        page["/Contents"] = pikepdf.Array(optimized_contents)
                except Exception as e:
                    raise ContentOptimizationError(
                        f"Failed to optimize content stream: {str(e)}",
                        traceback.format_exc(),
                    ) from e


def extreme_pdf_optimization(
    input_file: str,
    output_file: str,
    num_threads: int,
    grayscale: bool,
    flatten: bool,
) -> None:
    """
    Perform extreme optimization on a PDF file.

    Args:
        input_file (str): The path to the input PDF file.
        output_file (str): The path to save the optimized PDF file.
        num_threads (int): The number of threads to use for processing.
        grayscale (bool): Whether to convert images to grayscale.
        flatten (bool): Whether to flatten form fields and annotations.

    Raises:
        FileAccessError: If the input file is not found or cannot be accessed.
        PDFProcessingError: If PDF processing fails.
    """
    temp_files: List[str] = []

    try:
        logging.info("Opening PDF: %s", input_file)

        if not os.path.exists(input_file):
            raise FileAccessError(f"Input file not found: {input_file}", "")

        temp_pdf_path: str = create_temp_file(suffix=".pdf").name
        temp_files.append(temp_pdf_path)

        shutil.copy2(input_file, temp_pdf_path)

        pdf: pikepdf.Pdf = pikepdf.open(temp_pdf_path)
        doc: fitz.Document = fitz.open(temp_pdf_path)

        logging.info("Collecting used characters...")
        used_chars: Set[str] = get_used_characters(doc)

        logging.info("Aggressively subsetting fonts...")
        aggressive_font_subset(pdf, used_chars)

        logging.info("Removing metadata...")
        remove_metadata(pdf)

        logging.info("Cleaning PDF...")
        clean_pdf(pdf)

        if flatten:
            logging.info("Flattening annotations...")
            flatten_annotations(doc)

        logging.info("Optimizing content streams...")
        optimize_pdf_contents(pdf)

        temp_pdf_path2: str = create_temp_file(suffix=".pdf").name
        temp_files.append(temp_pdf_path2)

        pdf.save(temp_pdf_path2)
        pdf.close()

        doc = fitz.open(temp_pdf_path2)

        logging.info("Finding optimal vector simplification epsilon...")
        optimal_epsilon: float = find_optimal_vector_simplification(doc)
        if optimal_epsilon > 0.0:
            logging.info(
                "Optimal vector simplification epsilon: %f", optimal_epsilon
            )

            logging.info("Extremely optimizing vector paths...")
            for page in doc:
                page.clean_contents(sanitize=True)
                extreme_vector_optimization(page, optimal_epsilon)

        logging.info("Aggressively optimizing images...")
        process_images(doc, num_threads=num_threads, grayscale=grayscale)

        temp_pdf_path3: str = create_temp_file(suffix=".pdf").name
        temp_files.append(temp_pdf_path3)

        doc.save(temp_pdf_path3, garbage=4, deflate=True, clean=True)
        doc.close()

        logging.info("Applying final optimizations...")
        gs_params: List[str] = [
            "-sDEVICE=pdfwrite",
            "-dCompatibilityLevel=1.5",
            "-dPDFSETTINGS=/screen",
            "-dColorImageDownsampleType=/Bicubic",
            "-dColorImageResolution=72",
            "-dGrayImageDownsampleType=/Bicubic",
            "-dGrayImageResolution=72",
            "-dMonoImageDownsampleType=/Bicubic",
            "-dMonoImageResolution=72",
            "-dNOPAUSE",
            "-dQUIET",
            "-dBATCH",
            "-dEmbedAllFonts=true",
            "-dSubsetFonts=true",
            "-dCompressFonts=true",
            "-dOptimize=true",
            "-dUseFlateCompression=true",
            "-dAutoFilterColorImages=false",
            "-dAutoFilterGrayImages=false",
            "-dDownsampleMonoImages=false",
            "-dDownsampleGrayImages=false",
            "-dDownsampleColorImages=false",
        ]
        try:
            subprocess.run(
                ["gs"]
                + gs_params
                + [f"-sOutputFile={output_file}", temp_pdf_path3],
                check=True,
            )
        except subprocess.CalledProcessError as e:
            raise PDFProcessingError(
                f"Ghostscript optimization failed: {e.stderr}", ""
            ) from e

        logging.info(
            "Extreme optimization complete. Final file: %s", output_file
        )
    except Exception as e:
        logging.error("Error optimizing PDF: %s", str(e))
        logging.error(traceback.format_exc())
        raise PDFProcessingError("PDF optimization failed", str(e)) from e
    finally:
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)


def run(args: argparse.Namespace) -> None:
    """
    Run the PDF optimization process based on the provided arguments.

    Args:
        args (argparse.Namespace): The command-line arguments.

    Raises:
        PDFProcessingError: If PDF processing fails.
    """
    try:
        extreme_pdf_optimization(
            args.input_file,
            args.output_file,
            args.threads,
            args.grayscale,
            args.flatten,
        )

        original_size: int = os.path.getsize(args.input_file)
        optimized_size: int = os.path.getsize(args.output_file)
        reduction_percentage: float = (
            1 - optimized_size / original_size
        ) * 100
        logging.info("Original size: %d bytes", original_size)
        logging.info("Optimized size: %d bytes", optimized_size)
        logging.info("Size reduction: %.2f%%", reduction_percentage)
    except Exception as e:
        raise PDFProcessingError(
            "Error during PDF optimization", traceback.format_exc()
        ) from e


def optimize(args: argparse.Namespace) -> None:
    """
    Optimize the PDF file based on the provided arguments.

    Args:
        args (argparse.Namespace): The command-line arguments.
    """
    try:
        run(args)
    except FileAccessError as e:
        logging.error("File access error: %s", str(e))
        sys.exit(1)
    except PDFProcessingError as e:
        logging.error("PDF processing error: %s", str(e))
        sys.exit(1)
    except Exception as e:  # pylint: disable=W0703
        logging.error("An unexpected error occurred: %s", str(e))
        logging.error(traceback.format_exc())
        sys.exit(1)


def main() -> None:
    """Run the main optimization process."""
    args: argparse.Namespace = parse_arguments()
    try:
        optimize(args)
    except Exception as e:  # pylint: disable=W0703
        logging.error("An unexpected error occurred: %s", str(e))
        logging.error(traceback.format_exc())
        sys.exit(1)


def parse_arguments() -> argparse.Namespace:
    """
    Parse command-line arguments for the Advanced PDF Distiller.

    Returns:
        argparse.Namespace: The parsed arguments.
    """
    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Advanced PDF Distiller"
    )
    parser.add_argument("input_file", type=str, help="Input PDF file")
    parser.add_argument(
        "output_file", type=str, help="Output optimized PDF file"
    )
    parser.add_argument(
        "--threads",
        type=int,
        default=os.cpu_count(),
        help="Number of threads to use",
    )
    parser.add_argument(
        "--grayscale", action="store_true", help="Convert images to grayscale"
    )
    parser.add_argument(
        "--flatten",
        action="store_true",
        help="Flatten form fields and annotations",
    )
    return parser.parse_args()


if __name__ == "__main__":
    main()
